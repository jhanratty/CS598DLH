{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95b3ALuudB0O"
   },
   "source": [
    "## Instructions\n",
    "https://github.com/jhanratty/CS598DLH \n",
    "\n",
    "Clone the above repository to put TestCoxNerual_3.ipynb and data.zip into a directory.  \n",
    "\n",
    "If running in Colab, start TestCoxNeural_3.ipynb and edit the %cd {pwd} statement in first code cell with your working directory that contains the repo clone.  e.g. %cd '/content/drive/MyDrive/DLHProject/test2/'   \n",
    "    \n",
    "If not in Colab, cd to the working directory, start Start TestCoxNeural_3.ipynb.  \n",
    "\n",
    "Finally, in the notebook, 'run all'.  This will unzip the data.zip file to extract the dataset and requirements.txt files.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95b3ALuudB0O"
   },
   "source": [
    "## Verify Notebook Configuration and Find Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:13.994459Z",
     "start_time": "2022-05-07T14:06:13.983450Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LSmneeZx0AWh",
    "outputId": "a4735fe3-36fe-4ffd-f406-2c9159d1dc2e"
   },
   "outputs": [],
   "source": [
    "# IF IN COLAB, MOUNT GOOGLE DRIVE\n",
    "COLAB_HOME_DIRECTORY = '/content/drive/MyDrive/DLHProject/test2/'\n",
    "\n",
    "import sys\n",
    "import os\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    %cd '/content/drive/MyDrive/DLHProject/test2/'\n",
    "    %pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TO8qGahzI53U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip decompressed\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "## IF DATAFILE AND REQUIREMENTS.TXT NOT INSTALLED, INSTALL THEM\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "if (not os.path.exists('./seer_processed.csv')) or (not os.path.exists('./requirements.txt')):\n",
    "    if not os.path.exists('./data.zip'):\n",
    "        if 'wget' not in sys.modules:\n",
    "            %pip install wget\n",
    "        import wget\n",
    "        url = 'https://raw.githubusercontent.com/jhanratty/CS598DLH/main/data.zip'\n",
    "        filename = wget.download(url)\n",
    "        print('installed', filename)\n",
    "    with zipfile.ZipFile('./data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "    print(\"zip decompressed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:14.009972Z",
     "start_time": "2022-05-07T14:06:13.995460Z"
    },
    "id": "Yua0PqslQZoO"
   },
   "outputs": [],
   "source": [
    "#SET DIRECTORY LOCATION FOR NOTEBOOK AND DATA\n",
    "import os\n",
    "if (not os.path.exists('./seer_processed.csv')) or (not os.path.exists('./requirements.txt')):    \n",
    "    print('###### DATA FILES NOT FOUND ######')\n",
    "    print('Current Working Directory:', os.getcwd())\n",
    "    print('CURRENT DIRECTORY CONTENTS')\n",
    "    print(os.listdir('.'))\n",
    "    print('GitHub: https://github.com/jhanratty/CS598DLH   ')\n",
    "    assert False, \"PLEASE INSTALL data.zip FROM REPOSITORY IN CURRENT WORKING DIRECTORY\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9KUCAhFN_Xm"
   },
   "source": [
    "# REFERENCES\n",
    "Reference: Pycox Tutorial 02_Introduction  \n",
    "https://nbviewer.org/github/havakv/pycox/blob/master/examples/02_introduction.ipynb  \n",
    "https://github.com/havakv/pycox\n",
    "\n",
    "Explanation of Model  \n",
    "https://towardsdatascience.com/how-to-implement-deep-neural-networks-for-time-to-event-analyses-9aa0aeac4717 \n",
    "\n",
    "Torchtuples - used to format dataset  \n",
    "https://github.com/havakv/torchtuples \n",
    "\n",
    "Bert Model (Hugging Bear)  \n",
    "https://huggingface.co/transformers/v3.5.1/_modules/transformers/modeling_bert.html  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTvreUGiI53W"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:15.142445Z",
     "start_time": "2022-05-07T14:06:14.010973Z"
    },
    "id": "OAikP0BOI53W"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "if 'torch' not in sys.modules:\n",
    "    %pip install torch===1.5.0 torchvision===0.6.0 -f https://download.pytorch.org/whl/torch_stable.html --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:16.414537Z",
     "start_time": "2022-05-07T14:06:15.143946Z"
    },
    "id": "52MI1PtOI53X"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jdTCgwlbI53X"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "if 'torchtuples' not in sys.modules:\n",
    "    !pip install torchtuples\n",
    "if 'sksurv' not in sys.modules:\n",
    "    !pip install scikit-survival\n",
    "if 'pycox' not in sys.modules:\n",
    "    !pip install pycox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWkTFDyrJqU6"
   },
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:18.570889Z",
     "start_time": "2022-05-07T14:06:16.415538Z"
    },
    "id": "kcD0GfOewx3U"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn_pandas import DataFrameMapper \n",
    "\n",
    "import torch # For building the networks \n",
    "from torch import Tensor, device, dtype, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pdb\n",
    "import math, os\n",
    "from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "import inspect\n",
    "import warnings\n",
    "from easydict import EasyDict\n",
    "from collections import defaultdict\n",
    "from sksurv.metrics import concordance_index_ipcw, brier_score\n",
    "\n",
    "## PYCOX SURVIVAL ANALYSIS FUNCTIONS\n",
    "import torchtuples as tt\n",
    "from pycox.datasets import metabric, support\n",
    "from pycox.models import LogisticHazard\n",
    "from pycox.preprocessing.feature_transforms import OrderedCategoricalLong\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox.models.loss import NLLPCHazardLoss\n",
    "from pycox.preprocessing.discretization import (make_cuts, IdxDiscUnknownC, _values_if_series,\n",
    "    DiscretizeUnknownC, Duration2Idx)\n",
    "\n",
    "\n",
    "np.random.seed(1234)\n",
    "_ = torch.manual_seed(123)\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:18.586403Z",
     "start_time": "2022-05-07T14:06:18.571890Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsSyBoZaI53Z",
    "outputId": "a2498f39-fcd2-450a-ec78-a7bf7332ac6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on cpu\n"
     ]
    }
   ],
   "source": [
    "print(f'running on {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTNzq2dQMRLX"
   },
   "source": [
    "# Data Set Exploration\n",
    "\n",
    "## SEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:19.129369Z",
     "start_time": "2022-05-07T14:06:18.587403Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GbpIfe9ULjP-",
    "outputId": "1019f688-5f80-456c-e470-016cf818fd2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 476746 entries, 0 to 476745\n",
      "Data columns (total 21 columns):\n",
      " #   Column                                                Non-Null Count   Dtype  \n",
      "---  ------                                                --------------   -----  \n",
      " 0   Sex                                                   476746 non-null  int64  \n",
      " 1   Year of diagnosis                                     476746 non-null  int64  \n",
      " 2   Race recode (W, B, AI, API)                           476746 non-null  int64  \n",
      " 3   Histologic Type ICD-O-3                               476746 non-null  int64  \n",
      " 4   Laterality                                            476746 non-null  int64  \n",
      " 5   Sequence number                                       476746 non-null  int64  \n",
      " 6   ER Status Recode Breast Cancer (1990+)                476746 non-null  int64  \n",
      " 7   PR Status Recode Breast Cancer (1990+)                476746 non-null  int64  \n",
      " 8   Summary stage 2000 (1998-2017)                        476746 non-null  int64  \n",
      " 9   RX Summ--Surg Prim Site (1998+)                       476746 non-null  int64  \n",
      " 10  Reason no cancer-directed surgery                     476746 non-null  int64  \n",
      " 11  First malignant primary indicator                     476746 non-null  int64  \n",
      " 12  Diagnostic Confirmation                               476746 non-null  int64  \n",
      " 13  Median household income inflation adj to 2019         476746 non-null  int64  \n",
      " 14  Regional nodes examined (1988+)                       476746 non-null  float64\n",
      " 15  CS tumor size (2004-2015)                             476746 non-null  float64\n",
      " 16  Total number of benign/borderline tumors for patient  476746 non-null  float64\n",
      " 17  Total number of in situ/malignant tumors for patient  476746 non-null  float64\n",
      " 18  duration                                              476746 non-null  int64  \n",
      " 19  event_heart                                           476746 non-null  float64\n",
      " 20  event_breast                                          476746 non-null  float64\n",
      "dtypes: float64(6), int64(15)\n",
      "memory usage: 76.4 MB\n"
     ]
    }
   ],
   "source": [
    "PATH_DATA = \"./seer_processed.csv\"\n",
    "df = pd.read_csv(PATH_DATA)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:19.160395Z",
     "start_time": "2022-05-07T14:06:19.130370Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "id": "dk02n0UWM5__",
    "outputId": "6c3c00df-722b-4d65-bcf5-7238b7e4ee74"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Year of diagnosis</th>\n",
       "      <th>Race recode (W, B, AI, API)</th>\n",
       "      <th>Histologic Type ICD-O-3</th>\n",
       "      <th>Laterality</th>\n",
       "      <th>Sequence number</th>\n",
       "      <th>ER Status Recode Breast Cancer (1990+)</th>\n",
       "      <th>PR Status Recode Breast Cancer (1990+)</th>\n",
       "      <th>Summary stage 2000 (1998-2017)</th>\n",
       "      <th>RX Summ--Surg Prim Site (1998+)</th>\n",
       "      <th>...</th>\n",
       "      <th>First malignant primary indicator</th>\n",
       "      <th>Diagnostic Confirmation</th>\n",
       "      <th>Median household income inflation adj to 2019</th>\n",
       "      <th>Regional nodes examined (1988+)</th>\n",
       "      <th>CS tumor size (2004-2015)</th>\n",
       "      <th>Total number of benign/borderline tumors for patient</th>\n",
       "      <th>Total number of in situ/malignant tumors for patient</th>\n",
       "      <th>duration</th>\n",
       "      <th>event_heart</th>\n",
       "      <th>event_breast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.484635</td>\n",
       "      <td>-0.338709</td>\n",
       "      <td>-0.079797</td>\n",
       "      <td>0.941035</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.484635</td>\n",
       "      <td>-0.193297</td>\n",
       "      <td>-0.079797</td>\n",
       "      <td>0.941035</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4.834512</td>\n",
       "      <td>3.441995</td>\n",
       "      <td>-0.079797</td>\n",
       "      <td>2.454367</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.484635</td>\n",
       "      <td>-0.334882</td>\n",
       "      <td>-0.079797</td>\n",
       "      <td>2.454367</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.430906</td>\n",
       "      <td>-0.308096</td>\n",
       "      <td>-0.079797</td>\n",
       "      <td>0.941035</td>\n",
       "      <td>57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex  Year of diagnosis  Race recode (W, B, AI, API)  \\\n",
       "0    0                  1                            3   \n",
       "1    0                  1                            3   \n",
       "2    0                  9                            3   \n",
       "3    0                  8                            3   \n",
       "4    0                 10                            3   \n",
       "\n",
       "   Histologic Type ICD-O-3  Laterality  Sequence number  \\\n",
       "0                        1           1                1   \n",
       "1                        0           1                1   \n",
       "2                        5           4                2   \n",
       "3                        0           4                1   \n",
       "4                        0           4                1   \n",
       "\n",
       "   ER Status Recode Breast Cancer (1990+)  \\\n",
       "0                                       2   \n",
       "1                                       1   \n",
       "2                                       0   \n",
       "3                                       2   \n",
       "4                                       2   \n",
       "\n",
       "   PR Status Recode Breast Cancer (1990+)  Summary stage 2000 (1998-2017)  \\\n",
       "0                                       1                               2   \n",
       "1                                       1                               2   \n",
       "2                                       0                               1   \n",
       "3                                       2                               1   \n",
       "4                                       2                               1   \n",
       "\n",
       "   RX Summ--Surg Prim Site (1998+)  ...  First malignant primary indicator  \\\n",
       "0                                9  ...                                  0   \n",
       "1                                9  ...                                  0   \n",
       "2                                0  ...                                  0   \n",
       "3                                4  ...                                  0   \n",
       "4                               13  ...                                  0   \n",
       "\n",
       "   Diagnostic Confirmation  Median household income inflation adj to 2019  \\\n",
       "0                        3                                              8   \n",
       "1                        3                                              8   \n",
       "2                        3                                              8   \n",
       "3                        3                                              8   \n",
       "4                        3                                              8   \n",
       "\n",
       "   Regional nodes examined (1988+)  CS tumor size (2004-2015)  \\\n",
       "0                        -0.484635                  -0.338709   \n",
       "1                        -0.484635                  -0.193297   \n",
       "2                         4.834512                   3.441995   \n",
       "3                        -0.484635                  -0.334882   \n",
       "4                        -0.430906                  -0.308096   \n",
       "\n",
       "   Total number of benign/borderline tumors for patient  \\\n",
       "0                                          -0.079797      \n",
       "1                                          -0.079797      \n",
       "2                                          -0.079797      \n",
       "3                                          -0.079797      \n",
       "4                                          -0.079797      \n",
       "\n",
       "   Total number of in situ/malignant tumors for patient  duration  \\\n",
       "0                                           0.941035           81   \n",
       "1                                           0.941035            7   \n",
       "2                                           2.454367           28   \n",
       "3                                           2.454367           75   \n",
       "4                                           0.941035           57   \n",
       "\n",
       "   event_heart  event_breast  \n",
       "0          0.0           1.0  \n",
       "1          0.0           0.0  \n",
       "2          0.0           1.0  \n",
       "3          0.0           0.0  \n",
       "4          0.0           0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:19.346555Z",
     "start_time": "2022-05-07T14:06:19.161396Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "67Afl9n1PCkZ",
    "outputId": "abb157ea-d89f-434f-dee8-74eedb3e8052"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Year of diagnosis</th>\n",
       "      <th>Race recode (W, B, AI, API)</th>\n",
       "      <th>Histologic Type ICD-O-3</th>\n",
       "      <th>Laterality</th>\n",
       "      <th>Sequence number</th>\n",
       "      <th>ER Status Recode Breast Cancer (1990+)</th>\n",
       "      <th>PR Status Recode Breast Cancer (1990+)</th>\n",
       "      <th>Summary stage 2000 (1998-2017)</th>\n",
       "      <th>RX Summ--Surg Prim Site (1998+)</th>\n",
       "      <th>...</th>\n",
       "      <th>First malignant primary indicator</th>\n",
       "      <th>Diagnostic Confirmation</th>\n",
       "      <th>Median household income inflation adj to 2019</th>\n",
       "      <th>Regional nodes examined (1988+)</th>\n",
       "      <th>CS tumor size (2004-2015)</th>\n",
       "      <th>Total number of benign/borderline tumors for patient</th>\n",
       "      <th>Total number of in situ/malignant tumors for patient</th>\n",
       "      <th>duration</th>\n",
       "      <th>event_heart</th>\n",
       "      <th>event_breast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>476746.000000</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>4.767460e+05</td>\n",
       "      <td>4.767460e+05</td>\n",
       "      <td>4.767460e+05</td>\n",
       "      <td>4.767460e+05</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>476746.000000</td>\n",
       "      <td>476746.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.008317</td>\n",
       "      <td>6.344920</td>\n",
       "      <td>2.716732</td>\n",
       "      <td>1.430493</td>\n",
       "      <td>2.476191</td>\n",
       "      <td>4.400354</td>\n",
       "      <td>1.711253</td>\n",
       "      <td>1.589893</td>\n",
       "      <td>1.236218</td>\n",
       "      <td>8.744598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>2.995700</td>\n",
       "      <td>5.483381</td>\n",
       "      <td>-4.992988e-14</td>\n",
       "      <td>1.267458e-14</td>\n",
       "      <td>-2.526568e-14</td>\n",
       "      <td>9.707847e-14</td>\n",
       "      <td>67.414437</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.183525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.090817</td>\n",
       "      <td>2.787301</td>\n",
       "      <td>0.624318</td>\n",
       "      <td>4.576692</td>\n",
       "      <td>1.495183</td>\n",
       "      <td>2.438526</td>\n",
       "      <td>0.563609</td>\n",
       "      <td>0.607504</td>\n",
       "      <td>0.562386</td>\n",
       "      <td>8.035572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380547</td>\n",
       "      <td>0.177847</td>\n",
       "      <td>2.325253</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>31.453498</td>\n",
       "      <td>0.207743</td>\n",
       "      <td>0.387097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.846348e-01</td>\n",
       "      <td>-3.808020e-01</td>\n",
       "      <td>-7.979748e-02</td>\n",
       "      <td>-5.722965e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-4.309060e-01</td>\n",
       "      <td>-3.387091e-01</td>\n",
       "      <td>-7.979748e-02</td>\n",
       "      <td>-5.722965e-01</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-3.234485e-01</td>\n",
       "      <td>-3.042695e-01</td>\n",
       "      <td>-7.979748e-02</td>\n",
       "      <td>-5.722965e-01</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-1.075937e-03</td>\n",
       "      <td>-2.468701e-01</td>\n",
       "      <td>-7.979748e-02</td>\n",
       "      <td>9.410350e-01</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.834512e+00</td>\n",
       "      <td>3.441995e+00</td>\n",
       "      <td>5.785978e+01</td>\n",
       "      <td>2.818100e+01</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Sex  Year of diagnosis  Race recode (W, B, AI, API)  \\\n",
       "count  476746.000000      476746.000000                476746.000000   \n",
       "mean        0.008317           6.344920                     2.716732   \n",
       "std         0.090817           2.787301                     0.624318   \n",
       "min         0.000000           0.000000                     0.000000   \n",
       "25%         0.000000           5.000000                     3.000000   \n",
       "50%         0.000000           7.000000                     3.000000   \n",
       "75%         0.000000           9.000000                     3.000000   \n",
       "max         1.000000          10.000000                     3.000000   \n",
       "\n",
       "       Histologic Type ICD-O-3     Laterality  Sequence number  \\\n",
       "count            476746.000000  476746.000000    476746.000000   \n",
       "mean                  1.430493       2.476191         4.400354   \n",
       "std                   4.576692       1.495183         2.438526   \n",
       "min                   0.000000       0.000000         0.000000   \n",
       "25%                   0.000000       1.000000         1.000000   \n",
       "50%                   0.000000       1.000000         6.000000   \n",
       "75%                   1.000000       4.000000         6.000000   \n",
       "max                  74.000000       4.000000         6.000000   \n",
       "\n",
       "       ER Status Recode Breast Cancer (1990+)  \\\n",
       "count                           476746.000000   \n",
       "mean                                 1.711253   \n",
       "std                                  0.563609   \n",
       "min                                  0.000000   \n",
       "25%                                  2.000000   \n",
       "50%                                  2.000000   \n",
       "75%                                  2.000000   \n",
       "max                                  2.000000   \n",
       "\n",
       "       PR Status Recode Breast Cancer (1990+)  Summary stage 2000 (1998-2017)  \\\n",
       "count                           476746.000000                   476746.000000   \n",
       "mean                                 1.589893                        1.236218   \n",
       "std                                  0.607504                        0.562386   \n",
       "min                                  0.000000                        0.000000   \n",
       "25%                                  1.000000                        1.000000   \n",
       "50%                                  2.000000                        1.000000   \n",
       "75%                                  2.000000                        2.000000   \n",
       "max                                  2.000000                        2.000000   \n",
       "\n",
       "       RX Summ--Surg Prim Site (1998+)  ...  \\\n",
       "count                    476746.000000  ...   \n",
       "mean                          8.744598  ...   \n",
       "std                           8.035572  ...   \n",
       "min                           0.000000  ...   \n",
       "25%                           4.000000  ...   \n",
       "50%                           5.000000  ...   \n",
       "75%                          13.000000  ...   \n",
       "max                          47.000000  ...   \n",
       "\n",
       "       First malignant primary indicator  Diagnostic Confirmation  \\\n",
       "count                      476746.000000            476746.000000   \n",
       "mean                            0.824322                 2.995700   \n",
       "std                             0.380547                 0.177847   \n",
       "min                             0.000000                 0.000000   \n",
       "25%                             1.000000                 3.000000   \n",
       "50%                             1.000000                 3.000000   \n",
       "75%                             1.000000                 3.000000   \n",
       "max                             1.000000                 5.000000   \n",
       "\n",
       "       Median household income inflation adj to 2019  \\\n",
       "count                                  476746.000000   \n",
       "mean                                        5.483381   \n",
       "std                                         2.325253   \n",
       "min                                         0.000000   \n",
       "25%                                         4.000000   \n",
       "50%                                         6.000000   \n",
       "75%                                         8.000000   \n",
       "max                                         9.000000   \n",
       "\n",
       "       Regional nodes examined (1988+)  CS tumor size (2004-2015)  \\\n",
       "count                     4.767460e+05               4.767460e+05   \n",
       "mean                     -4.992988e-14               1.267458e-14   \n",
       "std                       1.000001e+00               1.000001e+00   \n",
       "min                      -4.846348e-01              -3.808020e-01   \n",
       "25%                      -4.309060e-01              -3.387091e-01   \n",
       "50%                      -3.234485e-01              -3.042695e-01   \n",
       "75%                      -1.075937e-03              -2.468701e-01   \n",
       "max                       4.834512e+00               3.441995e+00   \n",
       "\n",
       "       Total number of benign/borderline tumors for patient  \\\n",
       "count                                       4.767460e+05      \n",
       "mean                                       -2.526568e-14      \n",
       "std                                         1.000001e+00      \n",
       "min                                        -7.979748e-02      \n",
       "25%                                        -7.979748e-02      \n",
       "50%                                        -7.979748e-02      \n",
       "75%                                        -7.979748e-02      \n",
       "max                                         5.785978e+01      \n",
       "\n",
       "       Total number of in situ/malignant tumors for patient       duration  \\\n",
       "count                                       4.767460e+05     476746.000000   \n",
       "mean                                        9.707847e-14         67.414437   \n",
       "std                                         1.000001e+00         31.453498   \n",
       "min                                        -5.722965e-01          1.000000   \n",
       "25%                                        -5.722965e-01         48.000000   \n",
       "50%                                        -5.722965e-01         69.000000   \n",
       "75%                                         9.410350e-01         92.000000   \n",
       "max                                         2.818100e+01        121.000000   \n",
       "\n",
       "         event_heart   event_breast  \n",
       "count  476746.000000  476746.000000  \n",
       "mean        0.045200       0.183525  \n",
       "std         0.207743       0.387097  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCS0_8HELiU3"
   },
   "source": [
    "# DATA SET PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:19.362069Z",
     "start_time": "2022-05-07T14:06:19.348557Z"
    },
    "id": "54lHH4c7hoSR"
   },
   "outputs": [],
   "source": [
    "# Code from pycox preprocessing \n",
    "# https://github.com/havakv/pycox/blob/master/pycox/preprocessing/label_transforms.py#L150\n",
    "# Commented fields from survTRACE demo source code\n",
    "class LabTransPCHazard:\n",
    "    \"\"\"\n",
    "    Defining time intervals (`cuts`) needed for the `PCHazard` method [1].\n",
    "    One can either determine the cut points in form of passing an array to this class,\n",
    "    or one can obtain cut points based on the training data.\n",
    "    Arguments:\n",
    "        cuts {int, array} -- Defining cut points, either the number of cuts, or the actual cut points.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        scheme {str} -- Scheme used for discretization. Either 'equidistant' or 'quantiles'\n",
    "            (default: {'equidistant})\n",
    "        min_ {float} -- Starting duration (default: {0.})\n",
    "        dtype {str, dtype} -- dtype of discretization.\n",
    "    References:\n",
    "    [1] Håvard Kvamme and Ørnulf Borgan. Continuous and Discrete-Time Survival Prediction\n",
    "        with Neural Networks. arXiv preprint arXiv:1910.06724, 2019.\n",
    "        https://arxiv.org/pdf/1910.06724.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, cuts, scheme='equidistant', min_=0., dtype=None):\n",
    "        self._cuts = cuts\n",
    "        self._scheme = scheme\n",
    "        self._min = min_\n",
    "        self._dtype_init = dtype\n",
    "        self._predefined_cuts = False\n",
    "        self.cuts = None\n",
    "        if hasattr(cuts, '__iter__'):\n",
    "            if type(cuts) is list:\n",
    "                cuts = np.array(cuts)\n",
    "            self.cuts = cuts\n",
    "            self.idu = IdxDiscUnknownC(self.cuts)\n",
    "            assert dtype is None, \"Need `dtype` to be `None` for specified cuts\"\n",
    "            self._dtype = type(self.cuts[0])\n",
    "            self._dtype_init = self._dtype\n",
    "            self._predefined_cuts = True\n",
    "        else:\n",
    "            self._cuts += 1\n",
    "\n",
    "    def fit(self, durations, events):\n",
    "        self._dtype = self._dtype_init\n",
    "        if self._dtype is None:\n",
    "            if isinstance(durations[0], np.floating):\n",
    "                self._dtype = durations.dtype\n",
    "            else:\n",
    "                self._dtype = np.dtype('float64')\n",
    "        durations = durations.astype(self._dtype)\n",
    "        self.duc = DiscretizeUnknownC(self.cuts, right_censor=True, censor_side='right')\n",
    "        self.di = Duration2Idx(self.cuts)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, durations, events):\n",
    "        self.fit(durations, events)\n",
    "        return self.transform(durations, events)\n",
    "\n",
    "    def transform(self, durations, events):\n",
    "        durations = _values_if_series(durations)\n",
    "        durations = durations.astype(self._dtype)\n",
    "        events = _values_if_series(events)\n",
    "        dur_disc, events = self.duc.transform(durations, events)\n",
    "        idx_durations = self.di.transform(dur_disc)\n",
    "        cut_diff = np.diff(self.cuts)\n",
    "        assert (cut_diff > 0).all(), 'Cuts are not unique.'\n",
    "        t_frac = 1. - (dur_disc - durations) / cut_diff[idx_durations-1]\n",
    "        if idx_durations.min() == 0:\n",
    "            print(\"\"\"Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\"\"\")\n",
    "            t_frac[idx_durations == 0] = 0\n",
    "            events[idx_durations == 0] = 0\n",
    "        idx_durations = idx_durations - 1\n",
    "        return idx_durations.astype('int64'), events.astype('float32'), t_frac.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:19.393095Z",
     "start_time": "2022-05-07T14:06:19.363070Z"
    },
    "id": "SvwsgR60-HbX"
   },
   "outputs": [],
   "source": [
    "### Based on Pycox example code\n",
    "### https://nbviewer.org/github/havakv/pycox/blob/master/examples/02_introduction.ipynb\n",
    "\n",
    "# INITIALIZE GLOBAL VARIABLE FOR DATASETS\n",
    "DATASET = 'seer'\n",
    "HORIZONS = [.25, .5, .75]\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# global varibles set by preprocess()\n",
    "COLS_STANDARDIZE = None\n",
    "COLS_CATEGORICAL = None\n",
    "EVENT_LIST = None\n",
    "NUM_EVENTS = None\n",
    "NFEAT_NUM = None\n",
    "NFEAT_CAT = None\n",
    "NFEAT_ALL = None\n",
    "\n",
    "def preprocess(data_in = 'seer'):\n",
    "    global COLS_STANDARDIZE\n",
    "    global COLS_CATEGORICAL\n",
    "    global EVENT_LIST\n",
    "    global NUM_EVENTS\n",
    "    global NFEAT_NUM\n",
    "    global NFEAT_CAT\n",
    "    global NFEAT_ALL\n",
    "\n",
    "    if data_in == 'support':\n",
    "        from pycox.datasets import support\n",
    "        df = support.read_df()\n",
    "        COLS_STANDARDIZE =  ['x0', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13']\n",
    "        COLS_CATEGORICAL =  ['x1', 'x2', 'x3', 'x4', 'x5', 'x6']\n",
    "        EVENT_LIST = [\"event\"]\n",
    "        times = np.quantile(df[\"duration\"][df[\"event\"]==1.0], HORIZONS).tolist()\n",
    "        df_feat = df.drop([\"duration\",\"event\"],axis=1)\n",
    "\n",
    "    elif data_in == 'metabric':\n",
    "        from pycox.datasets import metabric\n",
    "        df = metabric.read_df()\n",
    "        COLS_STANDARDIZE = ['x0', 'x1', 'x2', 'x3', 'x8']\n",
    "        COLS_CATEGORICAL =  ['x4', 'x5', 'x6', 'x7']\n",
    "        EVENT_LIST = [\"event\"]\n",
    "        times = np.quantile(df[\"duration\"][df[\"event\"]==1.0], HORIZONS).tolist()\n",
    "        df_feat = df.drop([\"duration\",\"event\"],axis=1)\n",
    "\n",
    "    elif data_in == 'seer':\n",
    "        PATH_DATA = \"./seer_processed.csv\"\n",
    "        df = pd.read_csv(PATH_DATA)\n",
    "        COLS_CATEGORICAL = [\"Sex\", \"Year of diagnosis\", \"Race recode (W, B, AI, API)\", \"Histologic Type ICD-O-3\",\n",
    "                    \"Laterality\", \"Sequence number\", \"ER Status Recode Breast Cancer (1990+)\",\n",
    "                    \"PR Status Recode Breast Cancer (1990+)\", \"Summary stage 2000 (1998-2017)\",\n",
    "                    \"RX Summ--Surg Prim Site (1998+)\", \"Reason no cancer-directed surgery\", \"First malignant primary indicator\",\n",
    "                    \"Diagnostic Confirmation\", \"Median household income inflation adj to 2019\"]\n",
    "        COLS_STANDARDIZE = [\"Regional nodes examined (1988+)\", \"CS tumor size (2004-2015)\", \"Total number of benign/borderline tumors for patient\",\n",
    "                        \"Total number of in situ/malignant tumors for patient\",]\n",
    "        EVENT_LIST = [\"event_breast\", \"event_heart\"]\n",
    "        times = np.quantile(df[\"duration\"][df[\"event_breast\"]==1.0], HORIZONS).tolist()\n",
    "        df_feat = df.drop([\"duration\",\"event_breast\", \"event_heart\"],axis=1)\n",
    "\n",
    "    else:\n",
    "        print(\"&&&&&& UNRECOGNIZED DATASET\", DATASET)\n",
    "    \n",
    "    print('Dataset:', data_in, 'Quantiles(.25,.50,.75):', times)\n",
    "\n",
    "    NUM_EVENTS = len(EVENT_LIST)\n",
    "    NFEAT_NUM = len(COLS_STANDARDIZE)\n",
    "    NFEAT_CAT = len(COLS_CATEGORICAL)\n",
    "    NFEAT_ALL = NFEAT_NUM + NFEAT_CAT\n",
    "\n",
    "##############\n",
    "    df_feat_standardize = df_feat[COLS_STANDARDIZE]        \n",
    "    df_feat_standardize_disc = StandardScaler().fit_transform(df_feat_standardize)\n",
    "    df_feat_standardize_disc = pd.DataFrame(df_feat_standardize_disc, columns=COLS_STANDARDIZE)\n",
    "    df_feat = pd.concat([df_feat[COLS_CATEGORICAL], df_feat_standardize_disc], axis=1)\n",
    "\n",
    "    VOCAB_SIZE = 0\n",
    "    for _,feat in enumerate(COLS_CATEGORICAL):\n",
    "        df_feat[feat] = LabelEncoder().fit_transform(df_feat[feat]).astype(float) + VOCAB_SIZE\n",
    "        VOCAB_SIZE = df_feat[feat].max() + 1\n",
    "\n",
    "    max_duration_idx = df[\"duration\"].argmax()\n",
    "    d_test = df_feat.drop(max_duration_idx).sample(frac=0.3, random_state=1234)\n",
    "    d_train = df_feat.drop(d_test.index)\n",
    "    d_val = d_train.drop(max_duration_idx).sample(frac=.1, random_state=1234)\n",
    "    d_train = d_train.drop(d_val.index)\n",
    "\n",
    "    df_train = df.iloc[d_train.index]   # raw duration data\n",
    "    df_test = df.iloc[d_test.index]     # raw duration data\n",
    "    df_val = df.iloc[d_val.index]     # raw duration data\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    for col in COLS_STANDARDIZE:\n",
    "        d_train[col] = scaler.fit_transform(d_train[[col]])\n",
    "        d_test[col] =  scaler.fit_transform(d_test[[col]])\n",
    "        d_val[col] =   scaler.fit_transform(d_val[[col]])\n",
    "\n",
    "    x_train = tt.tuplefy(d_train[COLS_STANDARDIZE].to_numpy(dtype='float'),\n",
    "                         d_train[COLS_CATEGORICAL].to_numpy(dtype='long'))\n",
    "    x_test =  tt.tuplefy(d_test[COLS_STANDARDIZE].to_numpy(dtype='float'),\n",
    "                         d_test[COLS_CATEGORICAL].to_numpy('long'))\n",
    "    x_val =   tt.tuplefy(d_val[COLS_STANDARDIZE].to_numpy(dtype='float'),\n",
    "                         d_val[COLS_CATEGORICAL].to_numpy(dtype='long'))\n",
    "\n",
    "    ## CONVERT CONTINUOUS DURATION TIMES INTO DISCRETE QUANTILES\n",
    "    ## notes about alternativc above in LabelTransform() function\n",
    "    labtrans = LabTransPCHazard(cuts=np.array([df[\"duration\"].min()]+times+[df[\"duration\"].max()]))\n",
    "    \n",
    "    # Y tuple (duration, event)\n",
    "    # y_ TRANSFORM TO QUANTILE number from time in df \n",
    "    get_target = lambda df, event: (df['duration'].values, df[event].values)\n",
    "\n",
    "    y_train = pd.DataFrame()\n",
    "    y_val =   pd.DataFrame()\n",
    "    y_test =  pd.DataFrame()\n",
    "\n",
    "    for i, event in enumerate(EVENT_LIST):\n",
    "        labtrans.fit(*get_target(df.loc[df_train.index], event))\n",
    "        yt_train = labtrans.transform(*get_target(df.loc[d_train.index], event))     # yt_train = (discrete duration, event indicator)\n",
    "        yt_val =   labtrans.transform(*get_target(df.loc[d_val.index], event))       # yt_val = (discrete duration, event indicator)\n",
    "        yt_test =  labtrans.transform(*get_target(df.loc[d_test.index], event))      # yt_train = (discrete duration, event indicator)\n",
    "\n",
    "        event_name = \"event_{}\".format(i)\n",
    "        y_train[event_name] = yt_train[1]\n",
    "        y_val[event_name] = yt_val[1]\n",
    "        y_test[event_name] = yt_test[1]\n",
    "\n",
    "    # discretized duration (AFTER EVENTS)\n",
    "    y_train[NUM_EVENTS] = yt_train[0]         # discrete duration\n",
    "    y_val[NUM_EVENTS] = yt_val[0]              # discrete duration\n",
    "    y_test[NUM_EVENTS] = df.loc[df_test.index][\"duration\"]   # KEEPING original - not discrete\n",
    "    \n",
    "    # fraction of of period when event occurs \n",
    "    # ** used by NLLPCHazardLoss loss function in train() **\n",
    "    y_train[NUM_EVENTS+1] = yt_train[2]\n",
    "    y_val[NUM_EVENTS+1] =   yt_val[2]\n",
    "    y_test[NUM_EVENTS+1] =  yt_test[2]\n",
    "\n",
    "    ## CREATE TUPLES OF NUMPY MATRICES\n",
    "    tuple_train = tt.tuplefy(x_train, y_train.to_numpy())\n",
    "    tuple_val = tt.tuplefy(x_val, y_val.to_numpy())\n",
    "    tuple_test = tt.tuplefy(x_test, y_test.to_numpy())   ## added this - may not need transformed labels for y_test and tuple_test\n",
    "    ## CONVERT TO TENSORS\n",
    "    tuple_train = tuple_train.to_tensor()\n",
    "    tuple_val = tuple_val.to_tensor()\n",
    "    tuple_test = tuple_test.to_tensor()\n",
    "\n",
    "    #print('Tensor TRAIN SET', tuple_train.shapes())\n",
    "    #print('Tensor VAL SET',  tuple_val.shapes())\n",
    "    #print('Tensor TEST SET', tuple_test.shapes())\n",
    "\n",
    "    return df_train, df_test, df_val, tuple_train, tuple_val, tuple_test, labtrans\n",
    "\n",
    "\n",
    "# df_train, df_test, df_val, (x_train, y_train), (x_val, y_val), (x_test, y_test), labtrans = preprocess(DATASET)\n",
    "\n",
    "class CustomDataset(Dataset):    \n",
    "    def __init__(self, ds):\n",
    "        self.xnum = ds[0][0]\n",
    "        self.xcat = ds[0][1]\n",
    "        self.dsy = ds[1]   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dsy)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        xo = tt.tuplefy(self.xnum[index,:], self.xcat[index,:])\n",
    "        return tt.tuplefy(xo, self.dsy[index,:])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kri2KTTOBlCJ"
   },
   "source": [
    "# TRANSFORMER ENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0Dj6SgJJSRc"
   },
   "source": [
    "### CONFIGURATION (Only for BertEncoders)\n",
    "Variables are set for new routines in their relevant cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:19.408609Z",
     "start_time": "2022-05-07T14:06:19.394097Z"
    },
    "id": "Jb-aTNAtKQYL"
   },
   "outputs": [],
   "source": [
    "### Used only by BertEncoder(config)\n",
    "HIDDEN_SIZE = 16\n",
    "\n",
    "STConfig = EasyDict(\n",
    "    {\n",
    "        'hidden_size': HIDDEN_SIZE, # embedding size\n",
    "        'num_hidden_layers': 2, # num of transformers\n",
    "        'intermediate_size': 64, # intermediate layer size in transformer layer\n",
    "        'num_attention_heads': 2, # num of attention heads in transformer layer\n",
    "        'hidden_dropout_prob': 0.0,\n",
    "        'attention_probs_dropout_prob': 0.1,\n",
    "        'initializer_range': 0.001,\n",
    "        'layer_norm_eps': 1e-12,\n",
    "        'chunk_size_feed_forward': 0, # no use\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:19.439635Z",
     "start_time": "2022-05-07T14:06:19.410110Z"
    },
    "id": "Q6dJhhGBfmYO"
   },
   "outputs": [],
   "source": [
    "# Based on Hugging Bear Code: BertSelfAttention() and BertLayer() with modifications\n",
    "# https://huggingface.co/transformers/v3.5.1/_modules/transformers/modeling_bert.html \n",
    "\n",
    "def apply_chunking_to_forward(\n",
    "    forward_fn: Callable[..., torch.Tensor], chunk_size: int, chunk_dim: int, *input_tensors\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This function chunks the :obj:`input_tensors` into smaller input tensor parts of size :obj:`chunk_size` over the\n",
    "    dimension :obj:`chunk_dim`. It then applies a layer :obj:`forward_fn` to each chunk independently to save memory.\n",
    "\n",
    "    If the :obj:`forward_fn` is independent across the :obj:`chunk_dim` this function will yield the same result as\n",
    "    directly applying :obj:`forward_fn` to :obj:`input_tensors`.\n",
    "\n",
    "    Args:\n",
    "        forward_fn (:obj:`Callable[..., torch.Tensor]`):\n",
    "            The forward function of the model.\n",
    "        chunk_size (:obj:`int`):\n",
    "            The chunk size of a chunked tensor: :obj:`num_chunks = len(input_tensors[0]) / chunk_size`.\n",
    "        chunk_dim (:obj:`int`):\n",
    "            The dimension over which the :obj:`input_tensors` should be chunked.\n",
    "        input_tensors (:obj:`Tuple[torch.Tensor]`):\n",
    "            The input tensors of ``forward_fn`` which will be chunked\n",
    "\n",
    "    Returns:\n",
    "        :obj:`torch.Tensor`: A tensor with the same shape as the :obj:`forward_fn` would have given if applied`.\n",
    "\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        # rename the usual forward() fn to forward_chunk()\n",
    "        def forward_chunk(self, hidden_states):\n",
    "            hidden_states = self.decoder(hidden_states)\n",
    "            return hidden_states\n",
    "\n",
    "        # implement a chunked forward function\n",
    "        def forward(self, hidden_states):\n",
    "            return apply_chunking_to_forward(self.forward_chunk, self.chunk_size_lm_head, self.seq_len_dim, hidden_states)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(input_tensors) > 0, f\"{input_tensors} has to be a tuple/list of tensors\"\n",
    "    tensor_shape = input_tensors[0].shape[chunk_dim]\n",
    "    assert all(\n",
    "        input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n",
    "    ), \"All input tenors have to be of the same shape\"\n",
    "\n",
    "    # inspect.signature exist since python 3.5 and is a python method -> no problem with backward compatibility\n",
    "    num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)\n",
    "    if num_args_in_forward_chunk_fn != len(input_tensors):\n",
    "        raise ValueError(\n",
    "            f\"forward_chunk_fn expects {num_args_in_forward_chunk_fn} arguments, but only {len(input_tensors)} input \"\n",
    "            \"tensors are given\"\n",
    "        )\n",
    "\n",
    "    if chunk_size > 0:\n",
    "        if input_tensors[0].shape[chunk_dim] % chunk_size != 0:\n",
    "            raise ValueError(\n",
    "                f\"The dimension to be chunked {input_tensors[0].shape[chunk_dim]} has to be a multiple of the chunk \"\n",
    "                f\"size {chunk_size}\"\n",
    "            )\n",
    "\n",
    "        num_chunks = input_tensors[0].shape[chunk_dim] // chunk_size\n",
    "\n",
    "        # chunk input tensor into tuples\n",
    "        input_tensors_chunks = tuple(input_tensor.chunk(num_chunks, dim=chunk_dim) for input_tensor in input_tensors)\n",
    "        # apply forward fn to every tuple\n",
    "        output_chunks = tuple(forward_fn(*input_tensors_chunk) for input_tensors_chunk in zip(*input_tensors_chunks))\n",
    "        # concatenate output at same dimension\n",
    "        return torch.cat(output_chunks, dim=chunk_dim)\n",
    "\n",
    "    return forward_fn(*input_tensors)\n",
    "    \n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            seq_length = hidden_states.size()[1]\n",
    "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        \n",
    "        self.self_att = BertSelfAttention(config)\n",
    "        \n",
    "        self.att_output_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.att_output_LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.att_output_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.intermediate_act_fn = nn.functional.gelu\n",
    "        \n",
    "        self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.output_LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        \n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_outputs = self.self_att(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "        \n",
    "        attention_output = self.att_output_dense(self_outputs[0])\n",
    "        attention_output = self.att_output_dropout(attention_output)\n",
    "        attention_output = self.att_output_LayerNorm(attention_output + hidden_states)\n",
    "        \n",
    "        self_attention_outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        \n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        \n",
    "        intermediate_output = self.intermediate_dense(attention_output)\n",
    "        intermediate_output = self.intermediate_act_fn(intermediate_output)\n",
    "        \n",
    "        layer_output = self.output_dense(intermediate_output)\n",
    "        layer_output = self.output_dropout(layer_output)\n",
    "        layer_output = self.output_LayerNorm(layer_output + attention_output)\n",
    "        \n",
    "        return layer_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:19.455149Z",
     "start_time": "2022-05-07T14:06:19.440636Z"
    },
    "id": "x-jWF1Spbx4-"
   },
   "outputs": [],
   "source": [
    "# Based on Hugging Bear Code: BertEncoder with modifications\n",
    "# https://huggingface.co/transformers/v3.5.1/_modules/transformers/modeling_bert.html \n",
    "\n",
    "class STEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) \n",
    "                for _ in range(config.num_hidden_layers)])\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=True,\n",
    "        ):\n",
    "        hidden_states = hidden_states.to(DEVICE)\n",
    "        if attention_mask:\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "        if head_mask:\n",
    "            head_mask = head_mask.to(DEVICE)\n",
    "        # decide whether or not return attention and hidden states of all layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "\n",
    "        \n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "            \n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                layer_head_mask,\n",
    "                output_attentions,\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "        \n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        return tuple(\n",
    "            v\n",
    "            for v in [\n",
    "                hidden_states,\n",
    "                all_hidden_states,\n",
    "                all_self_attentions,\n",
    "            ]\n",
    "            if v is not None\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQPH2s6MCOrg"
   },
   "source": [
    "# SurvTRACE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:19.470662Z",
     "start_time": "2022-05-07T14:06:19.456650Z"
    },
    "id": "PWeioM7SzWKo"
   },
   "outputs": [],
   "source": [
    "class ST_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.first_run = False\n",
    "\n",
    "        self.output_attentions = False\n",
    "        self.output_hidden_states = False\n",
    "        # self.num_hidden_layers = HIDDEN_LAYERS\n",
    "\n",
    "        ##### Embeddings initialization\n",
    "        self.word_embeddings = nn.Embedding(VOCAB_SIZE, HIDDEN_SIZE)\n",
    "        self.num_embeddings = nn.Parameter(torch.randn(1, NFEAT_NUM, HIDDEN_SIZE), requires_grad=True)\n",
    "        self.num_embeddings.data.normal_(mean=0.0, std=0.001)\n",
    "\n",
    "        #### only for BertEncoder\n",
    "        STConfig.num_feature = NFEAT_ALL\n",
    "        STConfig.num_numerical_feature = NFEAT_NUM\n",
    "        STConfig.num_categorical_feature = NFEAT_CAT\n",
    "        self.encoder = STEncoder(STConfig)\n",
    "        ##########\n",
    "\n",
    "        ###### TSR NET initialization #####\n",
    "        intermediate_size = 64\n",
    "        out_feature = 4\n",
    "        tsr_net = []\n",
    "        w_init = lambda w: nn.init.kaiming_normal_(w, nonlinearity=\"relu\")\n",
    "        tsr_net.append(\n",
    "            tt.practical.DenseVanillaBlock(HIDDEN_SIZE*NFEAT_ALL, intermediate_size,\n",
    "                batch_norm=True, dropout=0.0, activation=nn.ReLU,\n",
    "                w_init_=w_init)\n",
    "        )\n",
    "        self.tsr_net = nn.Sequential(*tsr_net)\n",
    "\n",
    "        #### K Competing Event Nets initialization - 1 output net per event type #####\n",
    "        ce_net = []\n",
    "        for _ in range(NUM_EVENTS):\n",
    "            ce_net.append(nn.Linear(intermediate_size, out_feature))\n",
    "        self.ce_net = nn.ModuleList(ce_net)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        input_nums=None,\n",
    "        event=0):  # output the prediction for different competing events\n",
    "         \n",
    "        ### Enbeddings\n",
    "        inputs_embeds = self.word_embeddings(input_ids.to(torch.long))\n",
    "        num_embeddings =  (torch.unsqueeze(input_nums, 2) * self.num_embeddings).to(torch.float)\n",
    "        embedding_output = torch.cat((inputs_embeds, num_embeddings), axis=1).float().to(DEVICE)\n",
    "            \n",
    "        ### Transformer Encoder\n",
    "        encoder_outputs = self.encoder(embedding_output)\n",
    "        sequence_output = encoder_outputs[1][0]\n",
    "\n",
    "        # TSR Net\n",
    "        hidden_states = encoder_outputs[0]\n",
    "        hidden_states = hidden_states.flatten(start_dim=1)\n",
    "        hidden_states = self.tsr_net(hidden_states)\n",
    "\n",
    "        # Competing Event Nets\n",
    "        output = self.ce_net[event](hidden_states)\n",
    "\n",
    "        return sequence_output, output\n",
    "\n",
    "    def predict(self, x_cat, x_num, event=0):       \n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.forward(x_cat, x_num, event=event)[1]\n",
    "        return preds\n",
    "\n",
    "    def predict_survival(self, x_cat, x_num, event=0):\n",
    "        preds = self.predict(x_cat, x_num, event=event)\n",
    "        hazard = F.softplus(preds)\n",
    "        #add leading column of zero padding\n",
    "        pad = torch.zeros_like(hazard[:, :1])\n",
    "        hazard = torch.cat([pad, hazard], dim=1)\n",
    "        survival = hazard.cumsum(1).mul(-1).exp()\n",
    "        return survival\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rdyj4e2KAhq"
   },
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:19.486175Z",
     "start_time": "2022-05-07T14:06:19.471663Z"
    },
    "id": "lWs8WFcIghhG"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_data, val_data, weight_decay, learning_rate, n_epochs, verbose=True):\n",
    "# DATASET FORMAT:  (x_categorical, x_numeric), (y)    {numpy arrays}\n",
    "#   y[range(NUM_EVENTS)]: {events}\n",
    "#   y[NUM_EVENTS]: {duration}\n",
    "#   y[NUM_EVENTS+1]: {proportion}\n",
    "\n",
    "    # assign no weight decay on these parameters\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, \n",
    "        learning_rate, \n",
    "        weight_decay=weight_decay)\n",
    "\n",
    "    metrics = [NLLPCHazardLoss(),]\n",
    "\n",
    "    train_loss_list, val_loss_list = [], []\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (train_x, train_y) in enumerate(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            for event in range(NUM_EVENTS):   # events\n",
    "                t0 = train_x[0].to(DEVICE)\n",
    "                t1 = train_x[1].to(DEVICE)\n",
    "                train_y = train_y.to(DEVICE)\n",
    "\n",
    "                phi = model(input_ids=t1, input_nums=t0, event=event)\n",
    "                batch_loss += metrics[0](phi[1], \n",
    "                                      train_y[:, NUM_EVENTS].long(), \n",
    "                                      train_y[:, event].long(), \n",
    "                                      train_y[:,NUM_EVENTS+1].float())\n",
    "            batch_loss.backward()\n",
    "            optimizer.step() \n",
    "            epoch_loss += batch_loss.item()\n",
    "        train_loss_list.append(epoch_loss/ (batch_idx+1))\n",
    "\n",
    "        if val_data is not None:\n",
    "            model.eval()\n",
    "            batch_loss = 0\n",
    "            val_loss = 0\n",
    "            for val_idx, (val_x, val_y) in enumerate(val_data):\n",
    "                val_x0 = val_x[0].to(DEVICE)\n",
    "                val_x1 = val_x[1].to(DEVICE)\n",
    "                val_y  = val_y.to(DEVICE)\n",
    "                #batch_loss = 0\n",
    "                for event in range(NUM_EVENTS):   # events \n",
    "                    with torch.no_grad():\n",
    "                        phi_val = model(input_ids=val_x1, input_nums=val_x0, event=event)\n",
    "                        val_loss += metrics[0](phi_val[1],\n",
    "                                              val_y[:, NUM_EVENTS].long(),  \n",
    "                                              val_y[:, event].long(), \n",
    "                                              val_y[:, NUM_EVENTS+1].float())\n",
    "              \n",
    "            if verbose:\n",
    "                print('=====')\n",
    "                print('Epoch: {} \\t Training Loss:   {:.6f}  Validation Loss:   {:.6f}'.format(epoch+1, epoch_loss/(batch_idx+1), val_loss/(val_idx+1)))\n",
    "            val_loss_list.append(val_loss.item() / (val_idx+1))\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, epoch_loss))\n",
    "\n",
    "    return train_loss_list, val_loss_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjzPQk8II85X"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:06:19.501689Z",
     "start_time": "2022-05-07T14:06:19.487176Z"
    },
    "id": "wIFoAf_Zc0Nw"
   },
   "outputs": [],
   "source": [
    "class Evaluator1:\n",
    "    def __init__(self, df_train):  # df_train has raw duration & event\n",
    "        '''the input duration_train should be the raw durations (continuous),\n",
    "        NOT the discrete index of duration.\n",
    "        '''      \n",
    "        ## GRABBING ORIGINAL (UNPROCESSED) COLUMNS FROM DATA SET -\n",
    "        self.df_train_raw = df_train   # original data with durations\n",
    "        self.first_multi = True\n",
    "\n",
    "    def eval(self, model, test_x, test_raw, verbose=True):\n",
    "        # test_x: post processing\n",
    "        # test_y: df_test before durations quantized\n",
    "        df_train_all = self.df_train_raw\n",
    "        concord = []\n",
    "        brier = []\n",
    "        for edx, event in enumerate(EVENT_LIST):\n",
    "            get_target = lambda df: (df['duration'].values, df[event].values)\n",
    "            durations_train, events_train = get_target(df_train_all)\n",
    "            et_train = np.array([(events_train[i], durations_train[i]) for i in range(len(events_train))],\n",
    "                            dtype = [('e', bool), ('t', float)])\n",
    "\n",
    "            times = labtrans.cuts[1:-1]   # labtrans variable passed back from proprocessing()\n",
    "            horizons = HORIZONS           # defined in preprocessing()\n",
    "\n",
    "            surv = model.predict_survival(test_x[1].to(DEVICE), test_x[0].to(DEVICE))\n",
    "            risk = 1 - surv\n",
    "\n",
    "            durations_test, events_test = get_target(test_raw)\n",
    "            et_test = np.array([(events_test[i], durations_test[i]) for i in range(len(events_test))],\n",
    "                        dtype = [('e', bool), ('t', float)])\n",
    "\n",
    "            metric_dict = defaultdict(list)\n",
    "            brs = brier_score(et_train, et_test, surv.to('cpu').numpy()[:,1:-1], times)[1].tolist()\n",
    "            cis = []\n",
    "            for i, _ in enumerate(times):\n",
    "                cis.append(\n",
    "                    concordance_index_ipcw(et_train, et_test, estimate=risk[:, i+1].to('cpu').numpy(), tau=times[i])[0]\n",
    "                    )\n",
    "                metric_dict[f'{edx}_{horizons[i]}_ipcw'] = cis[i]\n",
    "                metric_dict[f'{edx}_{horizons[i]}_brier'] = brs[i]\n",
    "            concord.append(cis)\n",
    "            brier.append(brs)\n",
    "\n",
    "\n",
    "            if verbose:\n",
    "                if edx == 0:\n",
    "                    print(\"+++\", event, \"+++\", EPOCHS, \"EPOCHS\")\n",
    "                    print(\"25% Quantile: Concordance={:.4f}  Brier={:.4f}\".format(metric_dict['0_0.25_ipcw'], metric_dict['0_0.25_brier']))\n",
    "                    print(\"50% Quantile: Concordance={:.4f}  Brier={:.4f}\".format(metric_dict['0_0.5_ipcw'],  metric_dict['0_0.5_brier']))\n",
    "                    print(\"75% Quantile: Concordance={:.4f}  Brier={:.4f}\".format(metric_dict['0_0.75_ipcw'], metric_dict['0_0.75_brier']))\n",
    "                else:\n",
    "                    print(\"+++\", event, \"+++\", EPOCHS, \"EPOCHS\")\n",
    "                    print(\"25% Quantile: Concordance={:.4f}  Brier={:.4f}\".format(metric_dict['1_0.25_ipcw'], metric_dict['1_0.25_brier']))\n",
    "                    print(\"50% Quantile: Concordance={:.4f}  Brier={:.4f}\".format(metric_dict['1_0.5_ipcw'],  metric_dict['1_0.5_brier']))\n",
    "                    print(\"75% Quantile: Concordance={:.4f}  Brier={:.4f}\".format(metric_dict['1_0.75_ipcw'], metric_dict['1_0.75_brier']))\n",
    "\n",
    "\n",
    "        return concord, brier\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3s5lzyMSKRpb"
   },
   "source": [
    "# FULL RESULTS\n",
    "## Numberic Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:14:47.111615Z",
     "start_time": "2022-05-07T14:06:19.503190Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 871
    },
    "id": "xUJpOK5OIxVk",
    "outputId": "c9e82d74-8ff0-420a-9020-8b26aa81de3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: support Quantiles(.25,.50,.75): [14.0, 57.0, 250.25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\n",
      "Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\n",
      "Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\n",
      "Dataset: support\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.16s/it]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ event +++ 12 EPOCHS\n",
      "25% Quantile: Concordance=0.6288  Brier=0.1877\n",
      "50% Quantile: Concordance=0.6162  Brier=0.2994\n",
      "75% Quantile: Concordance=0.6115  Brier=0.3009\n",
      "===========\n",
      "Dataset: metabric Quantiles(.25,.50,.75): [42.68333435058594, 85.86666870117188, 145.33333587646484]\n",
      "Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\n",
      "Dataset: metabric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ event +++ 12 EPOCHS\n",
      "25% Quantile: Concordance=0.6484  Brier=0.1921\n",
      "50% Quantile: Concordance=0.6279  Brier=0.3716\n",
      "75% Quantile: Concordance=0.6071  Brier=0.4399\n",
      "===========\n",
      "Dataset: seer Quantiles(.25,.50,.75): [17.0, 34.0, 58.0]\n",
      "Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\n",
      "Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\n",
      "Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\n",
      "Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\n",
      "Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\n",
      "Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: seer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [43:54<00:00, 131.75s/it]\n"
     ]
    }
   ],
   "source": [
    "met = {}\n",
    "for ds, ep in [('support', 12), ('metabric', 12), ('seer', 20)]:\n",
    "    DATASET = ds\n",
    "    EPOCHS = ep\n",
    "    BATCH_SIZE = 1280\n",
    "    LR = 1e-3  #learning rate default= 1e-3\n",
    "    WT_DECAY = 1e-4  #weight decay defalt le-4\n",
    "\n",
    "    df_train, df_test, df_val, train_data, val_data, test_data, labtrans = preprocess(DATASET)\n",
    "    print('Dataset:', DATASET)\n",
    "    train_loader = torch.utils.data.DataLoader(CustomDataset(train_data), batch_size = BATCH_SIZE, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(CustomDataset(val_data), batch_size = BATCH_SIZE, shuffle=False)\n",
    "    model = ST_Model().to(DEVICE)\n",
    "    train_loss_list, val_loss_list = train(model, train_loader, val_loader, \n",
    "                                           weight_decay=WT_DECAY,\n",
    "                                           learning_rate=LR, n_epochs=EPOCHS,\n",
    "                                           verbose=False)\n",
    "    Eval = Evaluator1(df_train)\n",
    "    met[ds] = Eval.eval(model, test_data[0], df_test, verbose=True)\n",
    "    print('===========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtugG8BhsN3r"
   },
   "source": [
    "## Competing Events (SEER DATA SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:14:47.189181Z",
     "start_time": "2022-05-07T14:14:47.112616Z"
    },
    "id": "Ck1TchmXZ6qN"
   },
   "outputs": [],
   "source": [
    "## Results Table for Competing Events\n",
    "seerb_c_paper = [.904, .883, .866]\n",
    "seerh_c_paper = [.797, .788, .775,]\n",
    "seerb_b_paper = [.035, .060, .082]\n",
    "seerh_b_paper = [.008, .016, .028]\n",
    "deephith = [.763, .748, .724]\n",
    "deephitb = [.896, .875, .852]\n",
    "dsmh     = [.765, .761, .750]\n",
    "dsmb     = [.895, .873, .856]\n",
    "\n",
    "dd = pd.DataFrame([met['seer'][0][0],met['seer'][0][1],\n",
    "                   met['seer'][1][0],met['seer'][1][1],\n",
    "                    seerb_c_paper, seerh_c_paper,\n",
    "                    seerb_b_paper, seerh_b_paper],\n",
    "                  columns=pd.Index((['25%', '50%', '75%'])),\n",
    "                  index=pd.MultiIndex.from_product([['Project Results','SurvTRACE Paper'],['Concordance', 'Brier'],['Breast','Heart']]))# ,\n",
    "pd.set_option('colheader_justify', 'center')\n",
    "dd.T.style.format(precision=4)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:14:47.204695Z",
     "start_time": "2022-05-07T14:14:47.190182Z"
    },
    "id": "SZCgq73TVF0J"
   },
   "outputs": [],
   "source": [
    "## Results Table for Competing Events\n",
    "seerb_c_paper = [.904, .883, .866]\n",
    "seerh_c_paper = [.797, .788, .775,]\n",
    "seerb_b_paper = [.035, .060, .082]\n",
    "seerh_b_paper = [.008, .016, .028]\n",
    "deephith = [.763, .748, .724]\n",
    "deephitb = [.896, .875, .852]\n",
    "dsmh     = [.765, .761, .750]\n",
    "dsmb     = [.895, .873, .856]\n",
    "\n",
    "dd = pd.DataFrame([met['seer'][0][0],met['seer'][0][1],\n",
    "                   met['seer'][1][0],met['seer'][1][1],\n",
    "                    seerb_c_paper, seerh_c_paper,\n",
    "                    seerb_b_paper, seerh_b_paper],\n",
    "                  columns=pd.Index((['25%', '50%', '75%'])),\n",
    "                  index=pd.MultiIndex.from_product([['Results Concordance', 'Results Brier', 'SurvTRACE Concordance', 'SurvTRACE Brier'],['Breast','Heart']]))# ,\n",
    "dd.T.style.format(precision=4)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:14:47.452908Z",
     "start_time": "2022-05-07T14:14:47.205696Z"
    },
    "id": "G2Yc2OFbmA21"
   },
   "outputs": [],
   "source": [
    "# PLOT COMPETING EVENT RESULTS - SEER DATA SET\n",
    "ind = np.arange(3)\n",
    "width=.35\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "fig.subplots_adjust(hspace=.1, wspace=0.2)\n",
    "plt.setp(axs, xticks=[0.2, 1.2, 2.2], xticklabels=['25%', '50%', '75%'])\n",
    "plt.suptitle(\"Competing Events SEER Dataset\", fontsize=16)\n",
    "\n",
    "axs[0][0].bar(ind, met['seer'][0][0], width=width, label='Results')\n",
    "axs[0][0].bar(ind+width, seerb_c_paper, width=width, label='SurvTRACE Paper')\n",
    "axs[0][0].set_title('Breast Cancer', fontsize=10)\n",
    "axs[0][0].set_ylabel('Concordance')\n",
    "\n",
    "axs[0][1].bar(ind, met['seer'][0][1], width=width, label='Results')\n",
    "axs[0][1].bar(ind+width, seerh_c_paper, width=width, label='SurvTRACE Paper')\n",
    "axs[0][1].set_title('Heart Disease', fontsize=10)\n",
    "\n",
    "axs[1][0].bar(ind, met['seer'][1][0], width=width, label='Results')\n",
    "axs[1][0].bar(ind+width, seerb_b_paper, width=width, label='SurvTRACE Paper')\n",
    "axs[1][0].set_ylabel('Brier Score Loss')\n",
    "axs[1][0].set_xlabel('Duration Quantile')\n",
    "\n",
    "axs[1][1].bar(ind, met['seer'][1][1], width=width, label='Results')\n",
    "axs[1][1].bar(ind+width, seerh_b_paper, width=width, label='SurvTRACE Paper')\n",
    "axs[1][1].set_xlabel('Duration Quantile')\n",
    "\n",
    "l = plt.legend(loc=\"upper left\", prop={'size': 8})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhDyLX4LwE8B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mW4dqAlsZrf"
   },
   "source": [
    "## Single Event Results (SUPPORT & Metabric Data Sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:14:47.654581Z",
     "start_time": "2022-05-07T14:14:47.454410Z"
    },
    "id": "d3zZkvocph41"
   },
   "outputs": [],
   "source": [
    "# PLOT SINGLE EVENT RESULTS - SUPPORT & METABRIC DATA SETS\n",
    "meta_c_paper = [.728, .690, .655]\n",
    "supp_c_paper = [.670, .633, .617]\n",
    "meta_b_paper = [.110, .177, .219]\n",
    "supp_b_paper = [.134, .206, .230]\n",
    "\n",
    "ind = np.arange(3)\n",
    "width=.35\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "fig.subplots_adjust(hspace=.1, wspace=0.2)\n",
    "plt.setp(axs, xticks=[0.2, 1.2, 2.2], xticklabels=['25%', '50%', '75%'])\n",
    "plt.suptitle(\"Single Event Data Results\", fontsize=16)\n",
    "\n",
    "axs[0][0].bar(ind, met['metabric'][0][0], width=width, label='Results')\n",
    "axs[0][0].bar(ind+width, meta_c_paper, width=width, label='SurvTRACE Paper')\n",
    "axs[0][0].set_title('Metabric Dataset', fontsize=10)\n",
    "axs[0][0].set_ylabel('Concordance')\n",
    "\n",
    "axs[0][1].bar(ind, met['support'][0][0], width=width, label='Results')\n",
    "axs[0][1].bar(ind+width, supp_c_paper, width=width, label='SurvTRACE Paper')\n",
    "axs[0][1].set_title('Support Dataset', fontsize=10)\n",
    "\n",
    "axs[1][0].bar(ind, met['metabric'][1][0], width=width, label='Results')\n",
    "axs[1][0].bar(ind+width, meta_b_paper, width=width, label='SurvTRACE Paper')\n",
    "axs[1][0].set_ylabel('Brier Score Loss')\n",
    "axs[1][0].set_xlabel('Duration Quantile')\n",
    "\n",
    "\n",
    "axs[1][1].bar(ind, met['support'][1][0], width=width, label='Results')\n",
    "axs[1][1].bar(ind+width, supp_b_paper, width=width, label='SurvTRACE Paper')\n",
    "axs[1][1].set_xlabel('Duration Quantile')\n",
    "\n",
    "l = plt.legend(loc=\"upper left\", prop={'size': 8})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABTa4MvpwHIk"
   },
   "source": [
    "## Comparison to Other Multi-Event Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:14:47.670094Z",
     "start_time": "2022-05-07T14:14:47.655582Z"
    },
    "id": "CYTsBrLSdGQX"
   },
   "outputs": [],
   "source": [
    "## Results Table for Other Method\n",
    "dd = pd.DataFrame([met['seer'][0][0],met['seer'][0][1],\n",
    "                    seerb_c_paper, seerh_c_paper,\n",
    "                    deephitb, deephith,\n",
    "                    dsmb, dsmh],\n",
    "                  columns=pd.Index((['25%', '50%', '75%'])),\n",
    "                  index=pd.MultiIndex.from_product([['Testing    |', 'SurvTRACE    |', 'DeepHit    |', 'DSM         |'],['Breast','Heart']]))# ,\n",
    "dd.T.style.format(precision=4)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:14:47.825228Z",
     "start_time": "2022-05-07T14:14:47.671095Z"
    },
    "id": "rl9Lh40TJ7V1"
   },
   "outputs": [],
   "source": [
    "## Plot Comparison to Other Methods\n",
    "ind = np.arange(3)\n",
    "width=.2\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "fig.subplots_adjust(hspace=.1, wspace=0.2)\n",
    "fig.set_size_inches(8, 3)\n",
    "plt.setp(axs, xticks=[0.2, 1.2, 2.2], xticklabels=['25%', '50%', '75%'])\n",
    "\n",
    "axs[0].bar(ind, met['seer'][0][0], width=width, label='Results')\n",
    "axs[0].bar(ind+width, seerb_c_paper, width=width, label='SurvTRACE')\n",
    "axs[0].bar(ind+2*width, deephitb, width=width, label='DeepHit')\n",
    "axs[0].bar(ind+3*width, dsmb, width=width, label='DSM')\n",
    "axs[0].set_title('SEER Brest Cancer Event')\n",
    "axs[0].set_ylabel('Concordance')\n",
    "axs[0].set_ylim([.5, 1])\n",
    "\n",
    "axs[1].bar(ind, met['seer'][0][1], width=width, label='Results')\n",
    "axs[1].bar(ind+width, seerh_c_paper, width=width, label='SurvTRACE')\n",
    "axs[1].bar(ind+width*2, deephith, width=width, label='DeepHit')\n",
    "axs[1].bar(ind+width*3, dsmh, width=width, label='DSM')\n",
    "axs[1].set_title('SEER Heart Disease Event')\n",
    "\n",
    "l = plt.legend(loc=\"lower left\", prop={'size': 8},\n",
    "               facecolor=\"white\", framealpha=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjZLrC6asuaZ"
   },
   "source": [
    "# Parameter Grid Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T14:14:47.840741Z",
     "start_time": "2022-05-07T14:14:47.826729Z"
    },
    "id": "QwM0N8cCeobN"
   },
   "outputs": [],
   "source": [
    "# GRID PARAMETER TESTING\n",
    "run_grid = False\n",
    "if run_grid:\n",
    "    EPOCHS = 20\n",
    "    BATCH_SIZE = 1280\n",
    "    LR = 1e-3  #learning rate default= 1e-3\n",
    "    WT_DECAY = 1e-4  #weight decay defalt le-4\n",
    "\n",
    "    grid_data = pd.DataFrame()\n",
    "    # #heads, embe_size, Intermediate_size, hidden_layers, \n",
    "    for heads in [2]:\n",
    "        STConfig.num_attention_heads = heads\n",
    "        for hid_size in [16]:\n",
    "            STConfig.hidden_size = hid_size\n",
    "            HIDDEN_SIZE = hid_size\n",
    "            for num_hid_lay in [2]:\n",
    "                STConfig.num_hidden_layers = num_hid_lay\n",
    "                for im_size in [32]:\n",
    "                    STConfig.intermediate_size = im_size\n",
    "                    for hid_drop in [0,.1,.2]:\n",
    "                        STConfig.hidden_dropout_prob = hid_drop\n",
    "                        for lrate in [1e-2, 1e-3, 1e-4]:\n",
    "                            LR = lrate\n",
    "                            print('---')\n",
    "                            print('EPOCHS=', EPOCHS, 'BATCH=', BATCH_SIZE, 'LR=', lrate, 'Drop=', hid_drop)\n",
    "                            print('HID SIZE', hid_size, 'HID_LAYERS', num_hid_lay, \"IM_SIZE\", im_size)\n",
    "                            train_loader = torch.utils.data.DataLoader(\n",
    "                                CustomDataset(train_data), \n",
    "                                batch_size = BATCH_SIZE, \n",
    "                                shuffle=True)\n",
    "                            val_loader = torch.utils.data.DataLoader(\n",
    "                                CustomDataset(val_data), \n",
    "                                batch_size = BATCH_SIZE, \n",
    "                                shuffle=False)\n",
    "              \n",
    "                            model = ST_Model().to(DEVICE)\n",
    "                            train_loss_list, val_loss_list = train(\n",
    "                                model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                weight_decay=WT_DECAY,\n",
    "                                learning_rate=LR, \n",
    "                                n_epochs=EPOCHS, \n",
    "                                verbose=False)\n",
    "                            Eval = Evaluator1(df_train)\n",
    "                            Eval.eval(model, x_test, df_test, brier=True)\n",
    "                            print(' ')\n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TestCoxNeural_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
