{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestCoxNeural_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Verify Notebook Configuration and Find Data File"
      ],
      "metadata": {
        "id": "95b3ALuudB0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IF IN COLAB, MOUNT GOOGLE DRIVE\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSmneeZx0AWh",
        "outputId": "4d21d787-1526-49dc-c7a8-f01bba8df485"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SET DIRECTORY LOCATION FOR NOTEBOOK AND DATA\n",
        "%cd /content/drive/MyDrive/DLHProject/projecta\n",
        "%pwd\n",
        "\n",
        "import os\n",
        "if not os.path.exists('./data/seer_processed.csv'):\n",
        "  print('######DATA FILE NOT FOUND IN ./data  #####')\n",
        "  print('1. Verify you are in the right working directory')\n",
        "  print('   If not, change the %cd link above in this cell')\n",
        "  print('2. Verify that that the ./data/seer_processed.csv exists from this directory')\n",
        "  print('   If not, download data.zip from Gihub and unzip in working directory')\n",
        "  assert False, \"CAN NOT FIND DATA FILE\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yua0PqslQZoO",
        "outputId": "b1ca9e0c-834f-4ede-d45e-159e2a0db4fd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DLHProject/projecta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REFERENCES\n",
        "Reference: Pycox Tutorial 02_Introduction  \n",
        "https://nbviewer.org/github/havakv/pycox/blob/master/examples/02_introduction.ipynb  \n",
        "https://github.com/havakv/pycox\n",
        "\n",
        "Explanation of Model  \n",
        "https://towardsdatascience.com/how-to-implement-deep-neural-networks-for-time-to-event-analyses-9aa0aeac4717 \n",
        "\n",
        "Torchtuples - used to format dataset  \n",
        "https://github.com/havakv/torchtuples \n",
        "\n",
        "Bert Model (Hugging Bear)  \n",
        "https://huggingface.co/transformers/v3.5.1/_modules/transformers/modeling_bert.html  \n"
      ],
      "metadata": {
        "id": "S9KUCAhFN_Xm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Libraries"
      ],
      "metadata": {
        "id": "tWkTFDyrJqU6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kcD0GfOewx3U"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# For preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn_pandas import DataFrameMapper \n",
        "\n",
        "import torch # For building the networks \n",
        "from torch import Tensor, device, dtype, nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import pdb\n",
        "import math, os\n",
        "from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n",
        "import inspect\n",
        "import warnings\n",
        "from easydict import EasyDict\n",
        "from collections import defaultdict\n",
        "\n",
        "import sys\n",
        "if 'torchtuples' not in sys.modules:\n",
        "  !pip install torchtuples\n",
        "import torchtuples as tt # Some useful functions\n",
        "if 'sksurv' not in sys.modules:\n",
        "  !pip install scikit-survival\n",
        "\n",
        "if 'pycox' not in sys.modules:\n",
        "  !pip install pycox\n",
        "\n",
        "from sksurv.metrics import concordance_index_ipcw, brier_score\n",
        "\n",
        "## PYCOX SURVIVAL ANALYSIS FUNCTIONS\n",
        "from pycox.datasets import metabric, support\n",
        "from pycox.models import LogisticHazard\n",
        "from pycox.preprocessing.feature_transforms import OrderedCategoricalLong\n",
        "from pycox.evaluation import EvalSurv\n",
        "from pycox.models.loss import NLLPCHazardLoss\n",
        "from pycox.preprocessing.discretization import (make_cuts, IdxDiscUnknownC, _values_if_series,\n",
        "    DiscretizeUnknownC, Duration2Idx)\n",
        "\n",
        "\n",
        "np.random.seed(1234)\n",
        "_ = torch.manual_seed(123)\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Set Exploration\n",
        "\n",
        "## SEER"
      ],
      "metadata": {
        "id": "MTNzq2dQMRLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_DATA = \"./data/seer_processed.csv\"\n",
        "df = pd.read_csv(PATH_DATA)\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbpIfe9ULjP-",
        "outputId": "dbaec11f-be0b-4ba1-90bd-694e9e4e26ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 476746 entries, 0 to 476745\n",
            "Data columns (total 21 columns):\n",
            " #   Column                                                Non-Null Count   Dtype  \n",
            "---  ------                                                --------------   -----  \n",
            " 0   Sex                                                   476746 non-null  int64  \n",
            " 1   Year of diagnosis                                     476746 non-null  int64  \n",
            " 2   Race recode (W, B, AI, API)                           476746 non-null  int64  \n",
            " 3   Histologic Type ICD-O-3                               476746 non-null  int64  \n",
            " 4   Laterality                                            476746 non-null  int64  \n",
            " 5   Sequence number                                       476746 non-null  int64  \n",
            " 6   ER Status Recode Breast Cancer (1990+)                476746 non-null  int64  \n",
            " 7   PR Status Recode Breast Cancer (1990+)                476746 non-null  int64  \n",
            " 8   Summary stage 2000 (1998-2017)                        476746 non-null  int64  \n",
            " 9   RX Summ--Surg Prim Site (1998+)                       476746 non-null  int64  \n",
            " 10  Reason no cancer-directed surgery                     476746 non-null  int64  \n",
            " 11  First malignant primary indicator                     476746 non-null  int64  \n",
            " 12  Diagnostic Confirmation                               476746 non-null  int64  \n",
            " 13  Median household income inflation adj to 2019         476746 non-null  int64  \n",
            " 14  Regional nodes examined (1988+)                       476746 non-null  float64\n",
            " 15  CS tumor size (2004-2015)                             476746 non-null  float64\n",
            " 16  Total number of benign/borderline tumors for patient  476746 non-null  float64\n",
            " 17  Total number of in situ/malignant tumors for patient  476746 non-null  float64\n",
            " 18  duration                                              476746 non-null  int64  \n",
            " 19  event_heart                                           476746 non-null  float64\n",
            " 20  event_breast                                          476746 non-null  float64\n",
            "dtypes: float64(6), int64(15)\n",
            "memory usage: 76.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "dk02n0UWM5__",
        "outputId": "6e59d1ca-1728-47a1-a5ea-287b643752ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Sex  Year of diagnosis  Race recode (W, B, AI, API)  \\\n",
              "0    0                  1                            3   \n",
              "1    0                  1                            3   \n",
              "2    0                  9                            3   \n",
              "3    0                  8                            3   \n",
              "4    0                 10                            3   \n",
              "\n",
              "   Histologic Type ICD-O-3  Laterality  Sequence number  \\\n",
              "0                        1           1                1   \n",
              "1                        0           1                1   \n",
              "2                        5           4                2   \n",
              "3                        0           4                1   \n",
              "4                        0           4                1   \n",
              "\n",
              "   ER Status Recode Breast Cancer (1990+)  \\\n",
              "0                                       2   \n",
              "1                                       1   \n",
              "2                                       0   \n",
              "3                                       2   \n",
              "4                                       2   \n",
              "\n",
              "   PR Status Recode Breast Cancer (1990+)  Summary stage 2000 (1998-2017)  \\\n",
              "0                                       1                               2   \n",
              "1                                       1                               2   \n",
              "2                                       0                               1   \n",
              "3                                       2                               1   \n",
              "4                                       2                               1   \n",
              "\n",
              "   RX Summ--Surg Prim Site (1998+)  ...  First malignant primary indicator  \\\n",
              "0                                9  ...                                  0   \n",
              "1                                9  ...                                  0   \n",
              "2                                0  ...                                  0   \n",
              "3                                4  ...                                  0   \n",
              "4                               13  ...                                  0   \n",
              "\n",
              "   Diagnostic Confirmation  Median household income inflation adj to 2019  \\\n",
              "0                        3                                              8   \n",
              "1                        3                                              8   \n",
              "2                        3                                              8   \n",
              "3                        3                                              8   \n",
              "4                        3                                              8   \n",
              "\n",
              "   Regional nodes examined (1988+)  CS tumor size (2004-2015)  \\\n",
              "0                        -0.484635                  -0.338709   \n",
              "1                        -0.484635                  -0.193297   \n",
              "2                         4.834512                   3.441995   \n",
              "3                        -0.484635                  -0.334882   \n",
              "4                        -0.430906                  -0.308096   \n",
              "\n",
              "   Total number of benign/borderline tumors for patient  \\\n",
              "0                                          -0.079797      \n",
              "1                                          -0.079797      \n",
              "2                                          -0.079797      \n",
              "3                                          -0.079797      \n",
              "4                                          -0.079797      \n",
              "\n",
              "   Total number of in situ/malignant tumors for patient  duration  \\\n",
              "0                                           0.941035           81   \n",
              "1                                           0.941035            7   \n",
              "2                                           2.454367           28   \n",
              "3                                           2.454367           75   \n",
              "4                                           0.941035           57   \n",
              "\n",
              "   event_heart  event_breast  \n",
              "0          0.0           1.0  \n",
              "1          0.0           0.0  \n",
              "2          0.0           1.0  \n",
              "3          0.0           0.0  \n",
              "4          0.0           0.0  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6f56d484-2de9-42ee-ac61-4a8dd9d11c3a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Year of diagnosis</th>\n",
              "      <th>Race recode (W, B, AI, API)</th>\n",
              "      <th>Histologic Type ICD-O-3</th>\n",
              "      <th>Laterality</th>\n",
              "      <th>Sequence number</th>\n",
              "      <th>ER Status Recode Breast Cancer (1990+)</th>\n",
              "      <th>PR Status Recode Breast Cancer (1990+)</th>\n",
              "      <th>Summary stage 2000 (1998-2017)</th>\n",
              "      <th>RX Summ--Surg Prim Site (1998+)</th>\n",
              "      <th>...</th>\n",
              "      <th>First malignant primary indicator</th>\n",
              "      <th>Diagnostic Confirmation</th>\n",
              "      <th>Median household income inflation adj to 2019</th>\n",
              "      <th>Regional nodes examined (1988+)</th>\n",
              "      <th>CS tumor size (2004-2015)</th>\n",
              "      <th>Total number of benign/borderline tumors for patient</th>\n",
              "      <th>Total number of in situ/malignant tumors for patient</th>\n",
              "      <th>duration</th>\n",
              "      <th>event_heart</th>\n",
              "      <th>event_breast</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>-0.484635</td>\n",
              "      <td>-0.338709</td>\n",
              "      <td>-0.079797</td>\n",
              "      <td>0.941035</td>\n",
              "      <td>81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>-0.484635</td>\n",
              "      <td>-0.193297</td>\n",
              "      <td>-0.079797</td>\n",
              "      <td>0.941035</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4.834512</td>\n",
              "      <td>3.441995</td>\n",
              "      <td>-0.079797</td>\n",
              "      <td>2.454367</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>-0.484635</td>\n",
              "      <td>-0.334882</td>\n",
              "      <td>-0.079797</td>\n",
              "      <td>2.454367</td>\n",
              "      <td>75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>-0.430906</td>\n",
              "      <td>-0.308096</td>\n",
              "      <td>-0.079797</td>\n",
              "      <td>0.941035</td>\n",
              "      <td>57</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6f56d484-2de9-42ee-ac61-4a8dd9d11c3a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6f56d484-2de9-42ee-ac61-4a8dd9d11c3a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6f56d484-2de9-42ee-ac61-4a8dd9d11c3a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "67Afl9n1PCkZ",
        "outputId": "11289614-6ff2-477c-96aa-7d9881a1657e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Sex  Year of diagnosis  Race recode (W, B, AI, API)  \\\n",
              "count  476746.000000      476746.000000                476746.000000   \n",
              "mean        0.008317           6.344920                     2.716732   \n",
              "std         0.090817           2.787301                     0.624318   \n",
              "min         0.000000           0.000000                     0.000000   \n",
              "25%         0.000000           5.000000                     3.000000   \n",
              "50%         0.000000           7.000000                     3.000000   \n",
              "75%         0.000000           9.000000                     3.000000   \n",
              "max         1.000000          10.000000                     3.000000   \n",
              "\n",
              "       Histologic Type ICD-O-3     Laterality  Sequence number  \\\n",
              "count            476746.000000  476746.000000    476746.000000   \n",
              "mean                  1.430493       2.476191         4.400354   \n",
              "std                   4.576692       1.495183         2.438526   \n",
              "min                   0.000000       0.000000         0.000000   \n",
              "25%                   0.000000       1.000000         1.000000   \n",
              "50%                   0.000000       1.000000         6.000000   \n",
              "75%                   1.000000       4.000000         6.000000   \n",
              "max                  74.000000       4.000000         6.000000   \n",
              "\n",
              "       ER Status Recode Breast Cancer (1990+)  \\\n",
              "count                           476746.000000   \n",
              "mean                                 1.711253   \n",
              "std                                  0.563609   \n",
              "min                                  0.000000   \n",
              "25%                                  2.000000   \n",
              "50%                                  2.000000   \n",
              "75%                                  2.000000   \n",
              "max                                  2.000000   \n",
              "\n",
              "       PR Status Recode Breast Cancer (1990+)  Summary stage 2000 (1998-2017)  \\\n",
              "count                           476746.000000                   476746.000000   \n",
              "mean                                 1.589893                        1.236218   \n",
              "std                                  0.607504                        0.562386   \n",
              "min                                  0.000000                        0.000000   \n",
              "25%                                  1.000000                        1.000000   \n",
              "50%                                  2.000000                        1.000000   \n",
              "75%                                  2.000000                        2.000000   \n",
              "max                                  2.000000                        2.000000   \n",
              "\n",
              "       RX Summ--Surg Prim Site (1998+)  ...  \\\n",
              "count                    476746.000000  ...   \n",
              "mean                          8.744598  ...   \n",
              "std                           8.035572  ...   \n",
              "min                           0.000000  ...   \n",
              "25%                           4.000000  ...   \n",
              "50%                           5.000000  ...   \n",
              "75%                          13.000000  ...   \n",
              "max                          47.000000  ...   \n",
              "\n",
              "       First malignant primary indicator  Diagnostic Confirmation  \\\n",
              "count                      476746.000000            476746.000000   \n",
              "mean                            0.824322                 2.995700   \n",
              "std                             0.380547                 0.177847   \n",
              "min                             0.000000                 0.000000   \n",
              "25%                             1.000000                 3.000000   \n",
              "50%                             1.000000                 3.000000   \n",
              "75%                             1.000000                 3.000000   \n",
              "max                             1.000000                 5.000000   \n",
              "\n",
              "       Median household income inflation adj to 2019  \\\n",
              "count                                  476746.000000   \n",
              "mean                                        5.483381   \n",
              "std                                         2.325253   \n",
              "min                                         0.000000   \n",
              "25%                                         4.000000   \n",
              "50%                                         6.000000   \n",
              "75%                                         8.000000   \n",
              "max                                         9.000000   \n",
              "\n",
              "       Regional nodes examined (1988+)  CS tumor size (2004-2015)  \\\n",
              "count                     4.767460e+05               4.767460e+05   \n",
              "mean                     -4.992988e-14               1.267458e-14   \n",
              "std                       1.000001e+00               1.000001e+00   \n",
              "min                      -4.846348e-01              -3.808020e-01   \n",
              "25%                      -4.309060e-01              -3.387091e-01   \n",
              "50%                      -3.234485e-01              -3.042695e-01   \n",
              "75%                      -1.075937e-03              -2.468701e-01   \n",
              "max                       4.834512e+00               3.441995e+00   \n",
              "\n",
              "       Total number of benign/borderline tumors for patient  \\\n",
              "count                                       4.767460e+05      \n",
              "mean                                       -2.526568e-14      \n",
              "std                                         1.000001e+00      \n",
              "min                                        -7.979748e-02      \n",
              "25%                                        -7.979748e-02      \n",
              "50%                                        -7.979748e-02      \n",
              "75%                                        -7.979748e-02      \n",
              "max                                         5.785978e+01      \n",
              "\n",
              "       Total number of in situ/malignant tumors for patient       duration  \\\n",
              "count                                       4.767460e+05     476746.000000   \n",
              "mean                                        9.707847e-14         67.414437   \n",
              "std                                         1.000001e+00         31.453498   \n",
              "min                                        -5.722965e-01          1.000000   \n",
              "25%                                        -5.722965e-01         48.000000   \n",
              "50%                                        -5.722965e-01         69.000000   \n",
              "75%                                         9.410350e-01         92.000000   \n",
              "max                                         2.818100e+01        121.000000   \n",
              "\n",
              "         event_heart   event_breast  \n",
              "count  476746.000000  476746.000000  \n",
              "mean        0.045200       0.183525  \n",
              "std         0.207743       0.387097  \n",
              "min         0.000000       0.000000  \n",
              "25%         0.000000       0.000000  \n",
              "50%         0.000000       0.000000  \n",
              "75%         0.000000       0.000000  \n",
              "max         1.000000       1.000000  \n",
              "\n",
              "[8 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5557d0f1-75d5-4049-a4c5-15d660a6d87e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Year of diagnosis</th>\n",
              "      <th>Race recode (W, B, AI, API)</th>\n",
              "      <th>Histologic Type ICD-O-3</th>\n",
              "      <th>Laterality</th>\n",
              "      <th>Sequence number</th>\n",
              "      <th>ER Status Recode Breast Cancer (1990+)</th>\n",
              "      <th>PR Status Recode Breast Cancer (1990+)</th>\n",
              "      <th>Summary stage 2000 (1998-2017)</th>\n",
              "      <th>RX Summ--Surg Prim Site (1998+)</th>\n",
              "      <th>...</th>\n",
              "      <th>First malignant primary indicator</th>\n",
              "      <th>Diagnostic Confirmation</th>\n",
              "      <th>Median household income inflation adj to 2019</th>\n",
              "      <th>Regional nodes examined (1988+)</th>\n",
              "      <th>CS tumor size (2004-2015)</th>\n",
              "      <th>Total number of benign/borderline tumors for patient</th>\n",
              "      <th>Total number of in situ/malignant tumors for patient</th>\n",
              "      <th>duration</th>\n",
              "      <th>event_heart</th>\n",
              "      <th>event_breast</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>476746.000000</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>4.767460e+05</td>\n",
              "      <td>4.767460e+05</td>\n",
              "      <td>4.767460e+05</td>\n",
              "      <td>4.767460e+05</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>476746.000000</td>\n",
              "      <td>476746.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.008317</td>\n",
              "      <td>6.344920</td>\n",
              "      <td>2.716732</td>\n",
              "      <td>1.430493</td>\n",
              "      <td>2.476191</td>\n",
              "      <td>4.400354</td>\n",
              "      <td>1.711253</td>\n",
              "      <td>1.589893</td>\n",
              "      <td>1.236218</td>\n",
              "      <td>8.744598</td>\n",
              "      <td>...</td>\n",
              "      <td>0.824322</td>\n",
              "      <td>2.995700</td>\n",
              "      <td>5.483381</td>\n",
              "      <td>-4.992988e-14</td>\n",
              "      <td>1.267458e-14</td>\n",
              "      <td>-2.526568e-14</td>\n",
              "      <td>9.707847e-14</td>\n",
              "      <td>67.414437</td>\n",
              "      <td>0.045200</td>\n",
              "      <td>0.183525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.090817</td>\n",
              "      <td>2.787301</td>\n",
              "      <td>0.624318</td>\n",
              "      <td>4.576692</td>\n",
              "      <td>1.495183</td>\n",
              "      <td>2.438526</td>\n",
              "      <td>0.563609</td>\n",
              "      <td>0.607504</td>\n",
              "      <td>0.562386</td>\n",
              "      <td>8.035572</td>\n",
              "      <td>...</td>\n",
              "      <td>0.380547</td>\n",
              "      <td>0.177847</td>\n",
              "      <td>2.325253</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>31.453498</td>\n",
              "      <td>0.207743</td>\n",
              "      <td>0.387097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.846348e-01</td>\n",
              "      <td>-3.808020e-01</td>\n",
              "      <td>-7.979748e-02</td>\n",
              "      <td>-5.722965e-01</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>-4.309060e-01</td>\n",
              "      <td>-3.387091e-01</td>\n",
              "      <td>-7.979748e-02</td>\n",
              "      <td>-5.722965e-01</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>-3.234485e-01</td>\n",
              "      <td>-3.042695e-01</td>\n",
              "      <td>-7.979748e-02</td>\n",
              "      <td>-5.722965e-01</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>-1.075937e-03</td>\n",
              "      <td>-2.468701e-01</td>\n",
              "      <td>-7.979748e-02</td>\n",
              "      <td>9.410350e-01</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>4.834512e+00</td>\n",
              "      <td>3.441995e+00</td>\n",
              "      <td>5.785978e+01</td>\n",
              "      <td>2.818100e+01</td>\n",
              "      <td>121.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5557d0f1-75d5-4049-a4c5-15d660a6d87e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5557d0f1-75d5-4049-a4c5-15d660a6d87e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5557d0f1-75d5-4049-a4c5-15d660a6d87e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA SET PREPROSSING"
      ],
      "metadata": {
        "id": "vCS0_8HELiU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code from pycox preprocessing \n",
        "# https://github.com/havakv/pycox/blob/master/pycox/preprocessing/label_transforms.py#L150\n",
        "# Commented fields from survTRACE demo source code\n",
        "class LabTransPCHazard:\n",
        "    \"\"\"\n",
        "    Defining time intervals (`cuts`) needed for the `PCHazard` method [1].\n",
        "    One can either determine the cut points in form of passing an array to this class,\n",
        "    or one can obtain cut points based on the training data.\n",
        "    Arguments:\n",
        "        cuts {int, array} -- Defining cut points, either the number of cuts, or the actual cut points.\n",
        "    \n",
        "    Keyword Arguments:\n",
        "        scheme {str} -- Scheme used for discretization. Either 'equidistant' or 'quantiles'\n",
        "            (default: {'equidistant})\n",
        "        min_ {float} -- Starting duration (default: {0.})\n",
        "        dtype {str, dtype} -- dtype of discretization.\n",
        "    References:\n",
        "    [1] Håvard Kvamme and Ørnulf Borgan. Continuous and Discrete-Time Survival Prediction\n",
        "        with Neural Networks. arXiv preprint arXiv:1910.06724, 2019.\n",
        "        https://arxiv.org/pdf/1910.06724.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, cuts, scheme='equidistant', min_=0., dtype=None):\n",
        "        self._cuts = cuts\n",
        "        self._scheme = scheme\n",
        "        self._min = min_\n",
        "        self._dtype_init = dtype\n",
        "        self._predefined_cuts = False\n",
        "        self.cuts = None\n",
        "        if hasattr(cuts, '__iter__'):\n",
        "            if type(cuts) is list:\n",
        "                cuts = np.array(cuts)\n",
        "            self.cuts = cuts\n",
        "            self.idu = IdxDiscUnknownC(self.cuts)\n",
        "            assert dtype is None, \"Need `dtype` to be `None` for specified cuts\"\n",
        "            self._dtype = type(self.cuts[0])\n",
        "            self._dtype_init = self._dtype\n",
        "            self._predefined_cuts = True\n",
        "        else:\n",
        "            self._cuts += 1\n",
        "\n",
        "    def fit(self, durations, events):\n",
        "        self._dtype = self._dtype_init\n",
        "        if self._dtype is None:\n",
        "            if isinstance(durations[0], np.floating):\n",
        "                self._dtype = durations.dtype\n",
        "            else:\n",
        "                self._dtype = np.dtype('float64')\n",
        "        durations = durations.astype(self._dtype)\n",
        "        self.duc = DiscretizeUnknownC(self.cuts, right_censor=True, censor_side='right')\n",
        "        self.di = Duration2Idx(self.cuts)\n",
        "        return self\n",
        "\n",
        "    def fit_transform(self, durations, events):\n",
        "        self.fit(durations, events)\n",
        "        return self.transform(durations, events)\n",
        "\n",
        "    def transform(self, durations, events):\n",
        "        durations = _values_if_series(durations)\n",
        "        durations = durations.astype(self._dtype)\n",
        "        events = _values_if_series(events)\n",
        "        dur_disc, events = self.duc.transform(durations, events)\n",
        "        idx_durations = self.di.transform(dur_disc)\n",
        "        cut_diff = np.diff(self.cuts)\n",
        "        assert (cut_diff > 0).all(), 'Cuts are not unique.'\n",
        "        t_frac = 1. - (dur_disc - durations) / cut_diff[idx_durations-1]\n",
        "        if idx_durations.min() == 0:\n",
        "            warnings.warn(\"\"\"Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\"\"\")\n",
        "            t_frac[idx_durations == 0] = 0\n",
        "            events[idx_durations == 0] = 0\n",
        "        idx_durations = idx_durations - 1\n",
        "        return idx_durations.astype('int64'), events.astype('float32'), t_frac.astype('float32')\n"
      ],
      "metadata": {
        "id": "54lHH4c7hoSR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Based on Pycox example code\n",
        "### https://nbviewer.org/github/havakv/pycox/blob/master/examples/02_introduction.ipynb\n",
        "\n",
        "# INITIALIZE GLOBAL VARIABLE FOR DATASETS\n",
        "DATASET = 'seer'\n",
        "HORIZONS = [.25, .5, .75]\n",
        "VOCAB_SIZE = 1000\n",
        "\n",
        "# global varibles set by preprocess()\n",
        "COLS_STANDARDIZE = None\n",
        "COLS_CATEGORICAL = None\n",
        "EVENT_LIST = None\n",
        "NUM_EVENTS = None\n",
        "NFEAT_NUM = None\n",
        "NFEAT_CAT = None\n",
        "NFEAT_ALL = None\n",
        "\n",
        "def preprocess(data_in = 'seer'):\n",
        "    global COLS_STANDARDIZE\n",
        "    global COLS_CATEGORICAL\n",
        "    global EVENT_LIST\n",
        "    global NUM_EVENTS\n",
        "    global NFEAT_NUM\n",
        "    global NFEAT_CAT\n",
        "    global NFEAT_ALL\n",
        "\n",
        "    print('go0')\n",
        "    if data_in == 'support':\n",
        "      from pycox.datasets import support\n",
        "      df = support.read_df()\n",
        "      COLS_STANDARDIZE =  ['x0', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13']\n",
        "      COLS_CATEGORICAL =  ['x1', 'x2', 'x3', 'x4', 'x5', 'x6']\n",
        "      EVENT_LIST = [\"event\"]\n",
        "      times = np.quantile(df[\"duration\"][df[\"event\"]==1.0], HORIZONS).tolist()\n",
        "      df_feat = df.drop([\"duration\",\"event\"],axis=1)\n",
        "\n",
        "    elif data_in == 'metabric':\n",
        "      from pycox.datasets import metabric\n",
        "      df = metabric.read_df()\n",
        "      COLS_STANDARDIZE = ['x0', 'x1', 'x2', 'x3', 'x8']\n",
        "      COLS_CATEGORICAL =  ['x4', 'x5', 'x6', 'x7']\n",
        "      EVENT_LIST = [\"event\"]\n",
        "      times = np.quantile(df[\"duration\"][df[\"event\"]==1.0], HORIZONS).tolist()\n",
        "      df_feat = df.drop([\"duration\",\"event\"],axis=1)\n",
        "\n",
        "    elif data_in == 'seer':\n",
        "      PATH_DATA = \"./data/seer_processed.csv\"\n",
        "      df = pd.read_csv(PATH_DATA)\n",
        "      COLS_CATEGORICAL = [\"Sex\", \"Year of diagnosis\", \"Race recode (W, B, AI, API)\", \"Histologic Type ICD-O-3\",\n",
        "                    \"Laterality\", \"Sequence number\", \"ER Status Recode Breast Cancer (1990+)\",\n",
        "                    \"PR Status Recode Breast Cancer (1990+)\", \"Summary stage 2000 (1998-2017)\",\n",
        "                    \"RX Summ--Surg Prim Site (1998+)\", \"Reason no cancer-directed surgery\", \"First malignant primary indicator\",\n",
        "                    \"Diagnostic Confirmation\", \"Median household income inflation adj to 2019\"]\n",
        "      COLS_STANDARDIZE = [\"Regional nodes examined (1988+)\", \"CS tumor size (2004-2015)\", \"Total number of benign/borderline tumors for patient\",\n",
        "                        \"Total number of in situ/malignant tumors for patient\",]\n",
        "      EVENT_LIST = [\"event_breast\", \"event_heart\"]\n",
        "      times = np.quantile(df[\"duration\"][df[\"event_breast\"]==1.0], HORIZONS).tolist()\n",
        "      df_feat = df.drop([\"duration\",\"event_breast\", \"event_heart\"],axis=1)\n",
        "\n",
        "    else:\n",
        "      print(\"&&&&&& UNRECOGNIZED DATASET\", DATASET)\n",
        "    \n",
        "    print('Dataset:', data_in, 'Quantiles(.25,.50,.75):', times)\n",
        "\n",
        "    NUM_EVENTS = len(EVENT_LIST)\n",
        "    NFEAT_NUM = len(COLS_STANDARDIZE)\n",
        "    NFEAT_CAT = len(COLS_CATEGORICAL)\n",
        "    NFEAT_ALL = NFEAT_NUM + NFEAT_CAT\n",
        "\n",
        "##############\n",
        "    df_feat_standardize = df_feat[COLS_STANDARDIZE]        \n",
        "    df_feat_standardize_disc = StandardScaler().fit_transform(df_feat_standardize)\n",
        "    df_feat_standardize_disc = pd.DataFrame(df_feat_standardize_disc, columns=COLS_STANDARDIZE)\n",
        "    df_feat = pd.concat([df_feat[COLS_CATEGORICAL], df_feat_standardize_disc], axis=1)\n",
        "\n",
        "    VOCAB_SIZE = 0\n",
        "    for _,feat in enumerate(COLS_CATEGORICAL):\n",
        "        df_feat[feat] = LabelEncoder().fit_transform(df_feat[feat]).astype(float) + VOCAB_SIZE\n",
        "        VOCAB_SIZE = df_feat[feat].max() + 1\n",
        "\n",
        "    max_duration_idx = df[\"duration\"].argmax()\n",
        "    d_test = df_feat.drop(max_duration_idx).sample(frac=0.3, random_state=1234)\n",
        "    d_train = df_feat.drop(d_test.index)\n",
        "    d_val = d_train.drop(max_duration_idx).sample(frac=.1, random_state=1234)\n",
        "    d_train = d_train.drop(d_val.index)\n",
        "\n",
        "    df_train = df.iloc[d_train.index]   # raw duration data\n",
        "    df_test = df.iloc[d_test.index]     # raw duration data\n",
        "    df_val = df.iloc[d_val.index]     # raw duration data\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    for col in COLS_STANDARDIZE:\n",
        "        d_train[col] = scaler.fit_transform(d_train[[col]])\n",
        "        d_test[col] =  scaler.fit_transform(d_test[[col]])\n",
        "        d_val[col] =   scaler.fit_transform(d_val[[col]])\n",
        "\n",
        "    x_train = tt.tuplefy(d_train[COLS_STANDARDIZE].to_numpy(dtype='float'),\n",
        "                         d_train[COLS_CATEGORICAL].to_numpy(dtype='long'))\n",
        "    x_test =  tt.tuplefy(d_test[COLS_STANDARDIZE].to_numpy(dtype='float'),\n",
        "                         d_test[COLS_CATEGORICAL].to_numpy('long'))\n",
        "    x_val =   tt.tuplefy(d_val[COLS_STANDARDIZE].to_numpy(dtype='float'),\n",
        "                         d_val[COLS_CATEGORICAL].to_numpy(dtype='long'))\n",
        "\n",
        "    ## CONVERT CONTINUOUS DURATION TIMES INTO DISCRETE QUANTILES\n",
        "    ## notes about alternativc above in LabelTransform() function\n",
        "    labtrans = LabTransPCHazard(cuts=np.array([df[\"duration\"].min()]+times+[df[\"duration\"].max()]))\n",
        "    \n",
        "    # Y tuple (duration, event)\n",
        "    # y_ TRANSFORM TO QUANTILE number from time in df \n",
        "    get_target = lambda df, event: (df['duration'].values, df[event].values)\n",
        "\n",
        "    y_train = pd.DataFrame()\n",
        "    y_val =   pd.DataFrame()\n",
        "    y_test =  pd.DataFrame()\n",
        "\n",
        "    for i, event in enumerate(EVENT_LIST):\n",
        "        labtrans.fit(*get_target(df.loc[df_train.index], event))\n",
        "        yt_train = labtrans.transform(*get_target(df.loc[d_train.index], event))     # yt_train = (discrete duration, event indicator)\n",
        "        yt_val =   labtrans.transform(*get_target(df.loc[d_val.index], event))       # yt_val = (discrete duration, event indicator)\n",
        "        yt_test =  labtrans.transform(*get_target(df.loc[d_test.index], event))      # yt_train = (discrete duration, event indicator)\n",
        "\n",
        "        event_name = \"event_{}\".format(i)\n",
        "        y_train[event_name] = yt_train[1]\n",
        "        y_val[event_name] = yt_val[1]\n",
        "        y_test[event_name] = yt_test[1]\n",
        "\n",
        "    # discretized duration (AFTER EVENTS)\n",
        "    y_train[NUM_EVENTS] = yt_train[0]         # discrete duration\n",
        "    y_val[NUM_EVENTS] = yt_val[0]              # discrete duration\n",
        "    y_test[NUM_EVENTS] = df.loc[df_test.index][\"duration\"]   # KEEPING original - not discrete\n",
        "    \n",
        "    # fraction of of period when event occurs \n",
        "    # ** used by NLLPCHazardLoss loss function in train() **\n",
        "    y_train[NUM_EVENTS+1] = yt_train[2]\n",
        "    y_val[NUM_EVENTS+1] =   yt_val[2]\n",
        "    y_test[NUM_EVENTS+1] =  yt_test[2]\n",
        "\n",
        "    ## CREATE TUPLES OF NUMPY MATRICES\n",
        "    tuple_train = tt.tuplefy(x_train, y_train.to_numpy())\n",
        "    tuple_val = tt.tuplefy(x_val, y_val.to_numpy())\n",
        "    tuple_test = tt.tuplefy(x_test, y_test.to_numpy())   ## added this - may not need transformed labels for y_test and tuple_test\n",
        "    ## CONVERT TO TENSORS\n",
        "    tuple_train = tuple_train.to_tensor()\n",
        "    tuple_val = tuple_val.to_tensor()\n",
        "    tuple_test = tuple_test.to_tensor()\n",
        "\n",
        "    #print('Tensor TRAIN SET', tuple_train.shapes())\n",
        "    #print('Tensor VAL SET',  tuple_val.shapes())\n",
        "    #print('Tensor TEST SET', tuple_test.shapes())\n",
        "\n",
        "    return df_train, df_test, df_val, tuple_train, tuple_val, tuple_test, labtrans\n",
        "\n",
        "\n",
        "# df_train, df_test, df_val, (x_train, y_train), (x_val, y_val), (x_test, y_test), labtrans = preprocess(DATASET)\n",
        "\n",
        "class CustomDataset(Dataset):    \n",
        "    def __init__(self, ds):\n",
        "        self.xnum = ds[0][0]\n",
        "        self.xcat = ds[0][1]\n",
        "        self.dsy = ds[1]   \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dsy)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        xo = tt.tuplefy(self.xnum[index,:], self.xcat[index,:])\n",
        "        return tt.tuplefy(xo, self.dsy[index,:])\n",
        "        \n"
      ],
      "metadata": {
        "id": "SvwsgR60-HbX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRANSFORMER ENCODER"
      ],
      "metadata": {
        "id": "kri2KTTOBlCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONFIGURATION (Only for BertEncoders)\n",
        "Variables are set for new routines in their relevant cells"
      ],
      "metadata": {
        "id": "Y0Dj6SgJJSRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Used only by BertEncoder(config)\n",
        "HIDDEN_SIZE = 16\n",
        "\n",
        "STConfig = EasyDict(\n",
        "    {\n",
        "        'hidden_size': HIDDEN_SIZE, # embedding size\n",
        "        'num_hidden_layers': 2, # num of transformers\n",
        "        'intermediate_size': 64, # intermediate layer size in transformer layer\n",
        "        'num_attention_heads': 2, # num of attention heads in transformer layer\n",
        "        'hidden_dropout_prob': 0.0,\n",
        "        'attention_probs_dropout_prob': 0.1,\n",
        "        'initializer_range': 0.001,\n",
        "        'layer_norm_eps': 1e-12,\n",
        "        'chunk_size_feed_forward': 0, # no use\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "Jb-aTNAtKQYL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on Hugging Bear Code: BertSelfAttention() and BertLayer() with modifications\n",
        "# https://huggingface.co/transformers/v3.5.1/_modules/transformers/modeling_bert.html \n",
        "\n",
        "def apply_chunking_to_forward(\n",
        "    forward_fn: Callable[..., torch.Tensor], chunk_size: int, chunk_dim: int, *input_tensors\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This function chunks the :obj:`input_tensors` into smaller input tensor parts of size :obj:`chunk_size` over the\n",
        "    dimension :obj:`chunk_dim`. It then applies a layer :obj:`forward_fn` to each chunk independently to save memory.\n",
        "\n",
        "    If the :obj:`forward_fn` is independent across the :obj:`chunk_dim` this function will yield the same result as\n",
        "    directly applying :obj:`forward_fn` to :obj:`input_tensors`.\n",
        "\n",
        "    Args:\n",
        "        forward_fn (:obj:`Callable[..., torch.Tensor]`):\n",
        "            The forward function of the model.\n",
        "        chunk_size (:obj:`int`):\n",
        "            The chunk size of a chunked tensor: :obj:`num_chunks = len(input_tensors[0]) / chunk_size`.\n",
        "        chunk_dim (:obj:`int`):\n",
        "            The dimension over which the :obj:`input_tensors` should be chunked.\n",
        "        input_tensors (:obj:`Tuple[torch.Tensor]`):\n",
        "            The input tensors of ``forward_fn`` which will be chunked\n",
        "\n",
        "    Returns:\n",
        "        :obj:`torch.Tensor`: A tensor with the same shape as the :obj:`forward_fn` would have given if applied`.\n",
        "\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        # rename the usual forward() fn to forward_chunk()\n",
        "        def forward_chunk(self, hidden_states):\n",
        "            hidden_states = self.decoder(hidden_states)\n",
        "            return hidden_states\n",
        "\n",
        "        # implement a chunked forward function\n",
        "        def forward(self, hidden_states):\n",
        "            return apply_chunking_to_forward(self.forward_chunk, self.chunk_size_lm_head, self.seq_len_dim, hidden_states)\n",
        "    \"\"\"\n",
        "    \n",
        "    assert len(input_tensors) > 0, f\"{input_tensors} has to be a tuple/list of tensors\"\n",
        "    tensor_shape = input_tensors[0].shape[chunk_dim]\n",
        "    assert all(\n",
        "        input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n",
        "    ), \"All input tenors have to be of the same shape\"\n",
        "\n",
        "    # inspect.signature exist since python 3.5 and is a python method -> no problem with backward compatibility\n",
        "    num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)\n",
        "    if num_args_in_forward_chunk_fn != len(input_tensors):\n",
        "        raise ValueError(\n",
        "            f\"forward_chunk_fn expects {num_args_in_forward_chunk_fn} arguments, but only {len(input_tensors)} input \"\n",
        "            \"tensors are given\"\n",
        "        )\n",
        "\n",
        "    if chunk_size > 0:\n",
        "        if input_tensors[0].shape[chunk_dim] % chunk_size != 0:\n",
        "            raise ValueError(\n",
        "                f\"The dimension to be chunked {input_tensors[0].shape[chunk_dim]} has to be a multiple of the chunk \"\n",
        "                f\"size {chunk_size}\"\n",
        "            )\n",
        "\n",
        "        num_chunks = input_tensors[0].shape[chunk_dim] // chunk_size\n",
        "\n",
        "        # chunk input tensor into tuples\n",
        "        input_tensors_chunks = tuple(input_tensor.chunk(num_chunks, dim=chunk_dim) for input_tensor in input_tensors)\n",
        "        # apply forward fn to every tuple\n",
        "        output_chunks = tuple(forward_fn(*input_tensors_chunk) for input_tensors_chunk in zip(*input_tensors_chunks))\n",
        "        # concatenate output at same dimension\n",
        "        return torch.cat(output_chunks, dim=chunk_dim)\n",
        "\n",
        "    return forward_fn(*input_tensors)\n",
        "    \n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            seq_length = hidden_states.size()[1]\n",
        "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        \n",
        "        self.self_att = BertSelfAttention(config)\n",
        "        \n",
        "        self.att_output_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.att_output_LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.att_output_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        \n",
        "        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = nn.functional.gelu\n",
        "        \n",
        "        self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.output_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.output_LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        \n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_outputs = self.self_att(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "            output_attentions=output_attentions\n",
        "        )\n",
        "        \n",
        "        attention_output = self.att_output_dense(self_outputs[0])\n",
        "        attention_output = self.att_output_dropout(attention_output)\n",
        "        attention_output = self.att_output_LayerNorm(attention_output + hidden_states)\n",
        "        \n",
        "        self_attention_outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        \n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        layer_output = apply_chunking_to_forward(\n",
        "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
        "        )\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        \n",
        "        intermediate_output = self.intermediate_dense(attention_output)\n",
        "        intermediate_output = self.intermediate_act_fn(intermediate_output)\n",
        "        \n",
        "        layer_output = self.output_dense(intermediate_output)\n",
        "        layer_output = self.output_dropout(layer_output)\n",
        "        layer_output = self.output_LayerNorm(layer_output + attention_output)\n",
        "        \n",
        "        return layer_output\n",
        "\n"
      ],
      "metadata": {
        "id": "Q6dJhhGBfmYO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on Hugging Bear Code: BertEncoder with modifications\n",
        "# https://huggingface.co/transformers/v3.5.1/_modules/transformers/modeling_bert.html \n",
        "\n",
        "class STEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([BertLayer(config) \n",
        "                for _ in range(config.num_hidden_layers)])\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=True,\n",
        "        ):\n",
        "        hidden_states = hidden_states.to(DEVICE)\n",
        "        if attention_mask:\n",
        "            attention_mask = attention_mask.to(DEVICE)\n",
        "        if head_mask:\n",
        "            head_mask = head_mask.to(DEVICE)\n",
        "        # decide whether or not return attention and hidden states of all layers\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "\n",
        "        \n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "            \n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                layer_head_mask,\n",
        "                output_attentions,\n",
        "            )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "        \n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        return tuple(\n",
        "            v\n",
        "            for v in [\n",
        "                hidden_states,\n",
        "                all_hidden_states,\n",
        "                all_self_attentions,\n",
        "            ]\n",
        "            if v is not None\n",
        "        )\n"
      ],
      "metadata": {
        "id": "x-jWF1Spbx4-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SurvTRACE Model"
      ],
      "metadata": {
        "id": "YQPH2s6MCOrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ST_Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.first_run = False\n",
        "\n",
        "    self.output_attentions = False\n",
        "    self.output_hidden_states = False\n",
        "    # self.num_hidden_layers = HIDDEN_LAYERS\n",
        "\n",
        "    ##### Embeddings initialization\n",
        "    self.word_embeddings = nn.Embedding(VOCAB_SIZE, HIDDEN_SIZE)\n",
        "    self.num_embeddings = nn.Parameter(torch.randn(1, NFEAT_NUM, HIDDEN_SIZE), requires_grad=True)\n",
        "    self.num_embeddings.data.normal_(mean=0.0, std=0.001)\n",
        "\n",
        "    #### only for BertEncoder\n",
        "    STConfig.num_feature = NFEAT_ALL\n",
        "    STConfig.num_numerical_feature = NFEAT_NUM\n",
        "    STConfig.num_categorical_feature = NFEAT_CAT\n",
        "    self.encoder = STEncoder(STConfig)\n",
        "    ##########\n",
        "\n",
        "    ###### TSR NET initialization #####\n",
        "    intermediate_size = 64\n",
        "    out_feature = 4\n",
        "    tsr_net = []\n",
        "    w_init = lambda w: nn.init.kaiming_normal_(w, nonlinearity=\"relu\")\n",
        "    tsr_net.append(\n",
        "        tt.practical.DenseVanillaBlock(HIDDEN_SIZE*NFEAT_ALL, intermediate_size,\n",
        "            batch_norm=True, dropout=0.0, activation=nn.ReLU,\n",
        "            w_init_=w_init)\n",
        "    )\n",
        "    self.tsr_net = nn.Sequential(*tsr_net)\n",
        "\n",
        "    #### K Competing Event Nets initialization - 1 output net per event type #####\n",
        "    ce_net = []\n",
        "    for _ in range(NUM_EVENTS):\n",
        "        ce_net.append(nn.Linear(intermediate_size, out_feature))\n",
        "    self.ce_net = nn.ModuleList(ce_net)\n",
        "\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      input_ids=None,\n",
        "      input_nums=None,\n",
        "      event=0):  # output the prediction for different competing events\n",
        "       \n",
        "      ### Enbeddings\n",
        "      inputs_embeds = self.word_embeddings(input_ids)\n",
        "      num_embeddings =  torch.unsqueeze(input_nums, 2) * self.num_embeddings\n",
        "\n",
        "      embedding_output = torch.cat((inputs_embeds, num_embeddings), axis=1).float().to(DEVICE)\n",
        "          \n",
        "      ### Transformer Encoder\n",
        "      encoder_outputs = self.encoder(embedding_output)\n",
        "      sequence_output = encoder_outputs[1][0]\n",
        "\n",
        "      # TSR Net\n",
        "      hidden_states = encoder_outputs[0]\n",
        "      hidden_states = hidden_states.flatten(start_dim=1)\n",
        "      hidden_states = self.tsr_net(hidden_states)\n",
        "\n",
        "      # Competing Event Nets\n",
        "      output = self.ce_net[event](hidden_states)\n",
        "\n",
        "      return sequence_output, output\n",
        "\n",
        "  def predict(self, x_cat, x_num, event=0):       \n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            preds = self.forward(x_cat, x_num, event=event)[1]\n",
        "        return preds\n",
        "\n",
        "  def predict_survival(self, x_cat, x_num, event=0):\n",
        "    preds = self.predict(x_cat, x_num, event=event)\n",
        "    hazard = F.softplus(preds)\n",
        "    #add leading column of zero padding\n",
        "    pad = torch.zeros_like(hazard[:, :1])\n",
        "    hazard = torch.cat([pad, hazard], dim=1)\n",
        "    survival = hazard.cumsum(1).mul(-1).exp()\n",
        "    return survival\n"
      ],
      "metadata": {
        "id": "PWeioM7SzWKo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN MODEL"
      ],
      "metadata": {
        "id": "2rdyj4e2KAhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, train_data, val_data, weight_decay, learning_rate, n_epochs, verbose=True):\n",
        "# DATASET FORMAT:  (x_categorical, x_numeric), (y)    {numpy arrays}\n",
        "#   y[range(NUM_EVENTS)]: {events}\n",
        "#   y[NUM_EVENTS]: {duration}\n",
        "#   y[NUM_EVENTS+1]: {proportion}\n",
        "\n",
        "  # assign no weight decay on these parameters\n",
        "  no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  optimizer_grouped_parameters = [\n",
        "      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "  ]\n",
        "\n",
        "  optimizer = torch.optim.AdamW(optimizer_grouped_parameters, \n",
        "      learning_rate, \n",
        "     weight_decay=weight_decay)\n",
        "\n",
        "  metrics = [NLLPCHazardLoss(),]\n",
        "\n",
        "  train_loss_list, val_loss_list = [], []\n",
        "\n",
        "  for epoch in tqdm(range(n_epochs)):\n",
        "      model.train()\n",
        "      epoch_loss = 0\n",
        "      for batch_idx, (train_x, train_y) in enumerate(train_data):\n",
        "          optimizer.zero_grad()\n",
        "          batch_loss = 0\n",
        "          for event in range(NUM_EVENTS):   # events\n",
        "              t0 = train_x[0].to(DEVICE)\n",
        "              t1 = train_x[1].to(DEVICE)\n",
        "              train_y = train_y.to(DEVICE)\n",
        "\n",
        "              phi = model(input_ids=t1, input_nums=t0, event=event)\n",
        "              batch_loss += metrics[0](phi[1], \n",
        "                                      train_y[:, NUM_EVENTS].long(), \n",
        "                                      train_y[:, event].long(), \n",
        "                                      train_y[:,NUM_EVENTS+1].float())\n",
        "          batch_loss.backward()\n",
        "          optimizer.step() \n",
        "          epoch_loss += batch_loss.item()\n",
        "      train_loss_list.append(epoch_loss/ (batch_idx+1))\n",
        "\n",
        "      if val_data is not None:\n",
        "          model.eval()\n",
        "          batch_loss = 0\n",
        "          val_loss = 0\n",
        "          for val_idx, (val_x, val_y) in enumerate(val_data):\n",
        "              val_x0 = val_x[0].to(DEVICE)\n",
        "              val_x1 = val_x[1].to(DEVICE)\n",
        "              val_y  = val_y.to(DEVICE)\n",
        "              #batch_loss = 0\n",
        "              for event in range(NUM_EVENTS):   # events \n",
        "                  with torch.no_grad():\n",
        "                      phi_val = model(input_ids=val_x1, input_nums=val_x0, event=event)\n",
        "                      val_loss += metrics[0](phi_val[1],\n",
        "                                            val_y[:, NUM_EVENTS].long(),  \n",
        "                                            val_y[:, event].long(), \n",
        "                                            val_y[:, NUM_EVENTS+1].float())\n",
        "              \n",
        "          if verbose:\n",
        "            print('=====')\n",
        "            print('Epoch: {} \\t Training Loss:   {:.6f}  Validation Loss:   {:.6f}'.format(epoch+1, epoch_loss/(batch_idx+1), val_loss/(val_idx+1)))\n",
        "          val_loss_list.append(val_loss.item() / (val_idx+1))\n",
        "      else:\n",
        "          if verbose:\n",
        "            print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, epoch_loss))\n",
        "\n",
        "  return train_loss_list, val_loss_list\n",
        "\n"
      ],
      "metadata": {
        "id": "lWs8WFcIghhG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "MjzPQk8II85X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator1:\n",
        "    def __init__(self, df_train):  # df_train has raw duration & event\n",
        "        '''the input duration_train should be the raw durations (continuous),\n",
        "        NOT the discrete index of duration.\n",
        "        '''      \n",
        "        ## GRABBING ORIGINAL (UNPROCESSED) COLUMNS FROM DATA SET -\n",
        "        self.df_train_raw = df_train   # original data with durations\n",
        "        self.first_multi = True\n",
        "\n",
        "    def eval(self, model, test_x, test_raw, verbose=True):\n",
        "        # test_x: post processing\n",
        "        # test_y: df_test before durations quantized\n",
        "        df_train_all = self.df_train_raw\n",
        "        concord = []\n",
        "        brier = []\n",
        "        for edx, event in enumerate(EVENT_LIST):\n",
        "            get_target = lambda df: (df['duration'].values, df[event].values)\n",
        "            durations_train, events_train = get_target(df_train_all)\n",
        "            et_train = np.array([(events_train[i], durations_train[i]) for i in range(len(events_train))],\n",
        "                            dtype = [('e', bool), ('t', float)])\n",
        "\n",
        "            times = labtrans.cuts[1:-1]   # labtrans variable passed back from proprocessing()\n",
        "            horizons = HORIZONS           # defined in preprocessing()\n",
        "\n",
        "            surv = model.predict_survival(test_x[1].to(DEVICE), test_x[0].to(DEVICE))\n",
        "            risk = 1 - surv\n",
        "\n",
        "            durations_test, events_test = get_target(test_raw)\n",
        "            et_test = np.array([(events_test[i], durations_test[i]) for i in range(len(events_test))],\n",
        "                        dtype = [('e', bool), ('t', float)])\n",
        "\n",
        "            metric_dict = defaultdict(list)\n",
        "            brs = brier_score(et_train, et_test, surv.to(\"cpu\").numpy()[:,1:-1], times)[1].tolist()\n",
        "            cis = []\n",
        "            for i, _ in enumerate(times):\n",
        "                cis.append(\n",
        "                    concordance_index_ipcw(et_train, et_test, estimate=risk[:, i+1].to(\"cpu\").numpy(), tau=times[i])[0]\n",
        "                    )\n",
        "                metric_dict[f'{edx}_{horizons[i]}_ipcw'] = cis[i]\n",
        "                metric_dict[f'{edx}_{horizons[i]}_brier'] = brs[i]\n",
        "            concord.append(cis)\n",
        "            brier.append(brs)\n",
        "\n",
        "\n",
        "            if verbose:\n",
        "              if edx == 0:\n",
        "                print(\"+++\", event, \"+++\", EPOCHS, \"EPOCHS\")\n",
        "                print(\"25% Quantile: Concordance={:.4f}  Brier={:.4f}\".format(metric_dict['0_0.25_ipcw'], metric_dict['0_0.25_brier']))\n",
        "                print(\"50% Quantile: Concordance={:.4f}  Brier={:.4f}\".format(metric_dict['0_0.5_ipcw'],  metric_dict['0_0.5_brier']))\n",
        "                print(\"75% Quantile: Concordance={:.4f}  Brier={:.4f}\".format(metric_dict['0_0.75_ipcw'], metric_dict['0_0.75_brier']))\n",
        "              else:\n",
        "                print(\"+++\", event, \"+++\", EPOCHS, \"EPOCHS\")\n",
        "                print(\"25% Quantile: Concordance={:.4f}  Brier={:.4f}\".format(metric_dict['1_0.25_ipcw'], metric_dict['1_0.25_brier']))\n",
        "                print(\"50% Quantile: Concordance={:.4f}  Brier={:.4f}\".format(metric_dict['1_0.5_ipcw'],  metric_dict['1_0.5_brier']))\n",
        "                print(\"75% Quantile: Concordance={:.4f}  Brier={:.4f}\".format(metric_dict['1_0.75_ipcw'], metric_dict['1_0.75_brier']))\n",
        "\n",
        "\n",
        "        return concord, brier\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "wIFoAf_Zc0Nw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FULL RESULTS\n",
        "## Numberic Tables"
      ],
      "metadata": {
        "id": "3s5lzyMSKRpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "met = {}\n",
        "for ds, ep in [('support', 12), ('metabric', 12), ('seer', 20)]:\n",
        "    DATASET = ds\n",
        "    EPOCHS = ep\n",
        "    BATCH_SIZE = 1280\n",
        "    LR = 1e-3  #learning rate default= 1e-3\n",
        "    WT_DECAY = 1e-4  #weight decay defalt le-4\n",
        "\n",
        "    df_train, df_test, df_val, train_data, val_data, test_data, labtrans = preprocess(DATASET)\n",
        "    print('Dataset:', DATASET)\n",
        "    train_loader = torch.utils.data.DataLoader(CustomDataset(train_data), batch_size = BATCH_SIZE, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(CustomDataset(val_data), batch_size = BATCH_SIZE, shuffle=False)\n",
        "    model = ST_Model().to(DEVICE)\n",
        "    train_loss_list, val_loss_list = train(model, train_loader, val_loader, \n",
        "                                           weight_decay=WT_DECAY,\n",
        "                                           learning_rate=LR, n_epochs=EPOCHS,\n",
        "                                           verbose=False)\n",
        "    Eval = Evaluator1(df_train)\n",
        "    met[ds] = Eval.eval(model, test_data[0], df_test, verbose=True)\n",
        "    print('===========')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUJpOK5OIxVk",
        "outputId": "ac60da6b-dd00-4ba5-cda1-eb413a77c0b3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go0\n",
            "Dataset: support Quantiles(.25,.50,.75): [14.0, 57.0, 250.25]\n",
            "Dataset: support\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.64it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++ event +++ 12 EPOCHS\n",
            "25% Quantile: Concordance=0.6198  Brier=0.1969\n",
            "50% Quantile: Concordance=0.6095  Brier=0.3071\n",
            "75% Quantile: Concordance=0.6066  Brier=0.3087\n",
            "===========\n",
            "go0\n",
            "Dataset: metabric Quantiles(.25,.50,.75): [42.68333435058594, 85.86666870117188, 145.33333587646484]\n",
            "Dataset: metabric\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:00<00:00, 22.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++ event +++ 12 EPOCHS\n",
            "25% Quantile: Concordance=0.6765  Brier=0.2971\n",
            "50% Quantile: Concordance=0.6435  Brier=0.4205\n",
            "75% Quantile: Concordance=0.6256  Brier=0.4343\n",
            "===========\n",
            "go0\n",
            "Dataset: seer Quantiles(.25,.50,.75): [17.0, 34.0, 58.0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: seer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [04:14<00:00, 12.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++ event_breast +++ 20 EPOCHS\n",
            "25% Quantile: Concordance=0.9052  Brier=0.0358\n",
            "50% Quantile: Concordance=0.8831  Brier=0.0605\n",
            "75% Quantile: Concordance=0.8652  Brier=0.0821\n",
            "+++ event_heart +++ 20 EPOCHS\n",
            "25% Quantile: Concordance=0.7424  Brier=0.0130\n",
            "50% Quantile: Concordance=0.7109  Brier=0.0283\n",
            "75% Quantile: Concordance=0.6783  Brier=0.0479\n",
            "===========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Competing Events (SEER DATA SET)"
      ],
      "metadata": {
        "id": "FtugG8BhsN3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Results Table for Competing Events\n",
        "seerb_c_paper = [.904, .883, .866]\n",
        "seerh_c_paper = [.797, .788, .775,]\n",
        "seerb_b_paper = [.035, .060, .082]\n",
        "seerh_b_paper = [.008, .016, .028]\n",
        "deephith = [.763, .748, .724]\n",
        "deephitb = [.896, .875, .852]\n",
        "dsmh     = [.765, .761, .750]\n",
        "dsmb     = [.895, .873, .856]\n",
        "\n",
        "dd = pd.DataFrame([met['seer'][0][0],met['seer'][0][1],\n",
        "                   met['seer'][1][0],met['seer'][1][1],\n",
        "                    seerb_c_paper, seerh_c_paper,\n",
        "                    seerb_b_paper, seerh_b_paper],\n",
        "                  columns=pd.Index((['25%', '50%', '75%'])),\n",
        "                  index=pd.MultiIndex.from_product([['Project Results','SurvTRACE Paper'],['Concordance', 'Brier'],['Breast','Heart']]))# ,\n",
        "pd.set_option('colheader_justify', 'center')\n",
        "dd.T.style.format(precision=4)\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c53ad8b4-b56c-449b-b327-217547e5f20e",
        "id": "Ck1TchmXZ6qN"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f34f9724890>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_f6bb1_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" colspan=\"4\">Project Results</th>\n",
              "      <th class=\"col_heading level0 col4\" colspan=\"4\">SurvTRACE Paper</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"blank level1\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level1 col0\" colspan=\"2\">Concordance</th>\n",
              "      <th class=\"col_heading level1 col2\" colspan=\"2\">Brier</th>\n",
              "      <th class=\"col_heading level1 col4\" colspan=\"2\">Concordance</th>\n",
              "      <th class=\"col_heading level1 col6\" colspan=\"2\">Brier</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"blank level2\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level2 col0\" >Breast</th>\n",
              "      <th class=\"col_heading level2 col1\" >Heart</th>\n",
              "      <th class=\"col_heading level2 col2\" >Breast</th>\n",
              "      <th class=\"col_heading level2 col3\" >Heart</th>\n",
              "      <th class=\"col_heading level2 col4\" >Breast</th>\n",
              "      <th class=\"col_heading level2 col5\" >Heart</th>\n",
              "      <th class=\"col_heading level2 col6\" >Breast</th>\n",
              "      <th class=\"col_heading level2 col7\" >Heart</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_f6bb1_level0_row0\" class=\"row_heading level0 row0\" >25%</th>\n",
              "      <td id=\"T_f6bb1_row0_col0\" class=\"data row0 col0\" >0.9052</td>\n",
              "      <td id=\"T_f6bb1_row0_col1\" class=\"data row0 col1\" >0.7424</td>\n",
              "      <td id=\"T_f6bb1_row0_col2\" class=\"data row0 col2\" >0.0358</td>\n",
              "      <td id=\"T_f6bb1_row0_col3\" class=\"data row0 col3\" >0.0130</td>\n",
              "      <td id=\"T_f6bb1_row0_col4\" class=\"data row0 col4\" >0.9040</td>\n",
              "      <td id=\"T_f6bb1_row0_col5\" class=\"data row0 col5\" >0.7970</td>\n",
              "      <td id=\"T_f6bb1_row0_col6\" class=\"data row0 col6\" >0.0350</td>\n",
              "      <td id=\"T_f6bb1_row0_col7\" class=\"data row0 col7\" >0.0080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f6bb1_level0_row1\" class=\"row_heading level0 row1\" >50%</th>\n",
              "      <td id=\"T_f6bb1_row1_col0\" class=\"data row1 col0\" >0.8831</td>\n",
              "      <td id=\"T_f6bb1_row1_col1\" class=\"data row1 col1\" >0.7109</td>\n",
              "      <td id=\"T_f6bb1_row1_col2\" class=\"data row1 col2\" >0.0605</td>\n",
              "      <td id=\"T_f6bb1_row1_col3\" class=\"data row1 col3\" >0.0283</td>\n",
              "      <td id=\"T_f6bb1_row1_col4\" class=\"data row1 col4\" >0.8830</td>\n",
              "      <td id=\"T_f6bb1_row1_col5\" class=\"data row1 col5\" >0.7880</td>\n",
              "      <td id=\"T_f6bb1_row1_col6\" class=\"data row1 col6\" >0.0600</td>\n",
              "      <td id=\"T_f6bb1_row1_col7\" class=\"data row1 col7\" >0.0160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f6bb1_level0_row2\" class=\"row_heading level0 row2\" >75%</th>\n",
              "      <td id=\"T_f6bb1_row2_col0\" class=\"data row2 col0\" >0.8652</td>\n",
              "      <td id=\"T_f6bb1_row2_col1\" class=\"data row2 col1\" >0.6783</td>\n",
              "      <td id=\"T_f6bb1_row2_col2\" class=\"data row2 col2\" >0.0821</td>\n",
              "      <td id=\"T_f6bb1_row2_col3\" class=\"data row2 col3\" >0.0479</td>\n",
              "      <td id=\"T_f6bb1_row2_col4\" class=\"data row2 col4\" >0.8660</td>\n",
              "      <td id=\"T_f6bb1_row2_col5\" class=\"data row2 col5\" >0.7750</td>\n",
              "      <td id=\"T_f6bb1_row2_col6\" class=\"data row2 col6\" >0.0820</td>\n",
              "      <td id=\"T_f6bb1_row2_col7\" class=\"data row2 col7\" >0.0280</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Results Table for Competing Events\n",
        "seerb_c_paper = [.904, .883, .866]\n",
        "seerh_c_paper = [.797, .788, .775,]\n",
        "seerb_b_paper = [.035, .060, .082]\n",
        "seerh_b_paper = [.008, .016, .028]\n",
        "deephith = [.763, .748, .724]\n",
        "deephitb = [.896, .875, .852]\n",
        "dsmh     = [.765, .761, .750]\n",
        "dsmb     = [.895, .873, .856]\n",
        "\n",
        "dd = pd.DataFrame([met['seer'][0][0],met['seer'][0][1],\n",
        "                   met['seer'][1][0],met['seer'][1][1],\n",
        "                    seerb_c_paper, seerh_c_paper,\n",
        "                    seerb_b_paper, seerh_b_paper],\n",
        "                  columns=pd.Index((['25%', '50%', '75%'])),\n",
        "                  index=pd.MultiIndex.from_product([['Results Concordance', 'Results Brier', 'SurvTRACE Concordance', 'SurvTRACE Brier'],['Breast','Heart']]))# ,\n",
        "dd.T.style.format(precision=4)\n",
        " "
      ],
      "metadata": {
        "id": "SZCgq73TVF0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLOT COMPETING EVENT RESULTS - SEER DATA SET\n",
        "ind = np.arange(3)\n",
        "width=.35\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)\n",
        "fig.subplots_adjust(hspace=.1, wspace=0.2)\n",
        "plt.setp(axs, xticks=[0.2, 1.2, 2.2], xticklabels=['25%', '50%', '75%'])\n",
        "plt.suptitle(\"Competing Events SEER Dataset\", fontsize=16)\n",
        "\n",
        "axs[0][0].bar(ind, met['seer'][0][0], width=width, label='Results')\n",
        "axs[0][0].bar(ind+width, seerb_c_paper, width=width, label='SurvTRACE Paper')\n",
        "axs[0][0].set_title('Breast Cancer', fontsize=10)\n",
        "axs[0][0].set_ylabel('Concordance')\n",
        "\n",
        "axs[0][1].bar(ind, met['seer'][0][1], width=width, label='Results')\n",
        "axs[0][1].bar(ind+width, seerh_c_paper, width=width, label='SurvTRACE Paper')\n",
        "axs[0][1].set_title('Heart Disease', fontsize=10)\n",
        "\n",
        "axs[1][0].bar(ind, met['seer'][1][0], width=width, label='Results')\n",
        "axs[1][0].bar(ind+width, seerb_b_paper, width=width, label='SurvTRACE Paper')\n",
        "axs[1][0].set_ylabel('Brier Score Loss')\n",
        "axs[1][0].set_xlabel('Duration Quantile')\n",
        "\n",
        "axs[1][1].bar(ind, met['seer'][1][1], width=width, label='Results')\n",
        "axs[1][1].bar(ind+width, seerh_b_paper, width=width, label='SurvTRACE Paper')\n",
        "axs[1][1].set_xlabel('Duration Quantile')\n",
        "\n",
        "l = plt.legend(loc=\"upper left\", prop={'size': 8})\n"
      ],
      "metadata": {
        "id": "G2Yc2OFbmA21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "01370f54-e502-4b68-d056-b864c7140c4a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LhDyLX4LwE8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Event Results (SUPPORT & Metabric Data Sets)"
      ],
      "metadata": {
        "id": "4mW4dqAlsZrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PLOT SINGLE EVENT RESULTS - SUPPORT & METABRIC DATA SETS\n",
        "meta_c_paper = [.728, .690, .655]\n",
        "supp_c_paper = [.670, .633, .617]\n",
        "meta_b_paper = [.110, .177, .219]\n",
        "supp_b_paper = [.134, .206, .230]\n",
        "\n",
        "ind = np.arange(3)\n",
        "width=.35\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)\n",
        "fig.subplots_adjust(hspace=.1, wspace=0.2)\n",
        "plt.setp(axs, xticks=[0.2, 1.2, 2.2], xticklabels=['25%', '50%', '75%'])\n",
        "plt.suptitle(\"Single Event Data Results\", fontsize=16)\n",
        "\n",
        "axs[0][0].bar(ind, met['metabric'][0][0], width=width, label='Results')\n",
        "axs[0][0].bar(ind+width, meta_c_paper, width=width, label='SurvTRACE Paper')\n",
        "axs[0][0].set_title('Metabric Dataset', fontsize=10)\n",
        "axs[0][0].set_ylabel('Concordance')\n",
        "\n",
        "axs[0][1].bar(ind, met['support'][0][0], width=width, label='Results')\n",
        "axs[0][1].bar(ind+width, supp_c_paper, width=width, label='SurvTRACE Paper')\n",
        "axs[0][1].set_title('Support Dataset', fontsize=10)\n",
        "\n",
        "axs[1][0].bar(ind, met['metabric'][1][0], width=width, label='Results')\n",
        "axs[1][0].bar(ind+width, meta_b_paper, width=width, label='SurvTRACE Paper')\n",
        "axs[1][0].set_ylabel('Brier Score Loss')\n",
        "axs[1][0].set_xlabel('Duration Quantile')\n",
        "\n",
        "\n",
        "axs[1][1].bar(ind, met['support'][1][0], width=width, label='Results')\n",
        "axs[1][1].bar(ind+width, supp_b_paper, width=width, label='SurvTRACE Paper')\n",
        "axs[1][1].set_xlabel('Duration Quantile')\n",
        "\n",
        "l = plt.legend(loc=\"upper left\", prop={'size': 8})\n"
      ],
      "metadata": {
        "id": "d3zZkvocph41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison to Other Multi-Event Models"
      ],
      "metadata": {
        "id": "ABTa4MvpwHIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Results Table for Other Method\n",
        "dd = pd.DataFrame([met['seer'][0][0],met['seer'][0][1],\n",
        "                    seerb_c_paper, seerh_c_paper,\n",
        "                    deephitb, deephith,\n",
        "                    dsmb, dsmh],\n",
        "                  columns=pd.Index((['25%', '50%', '75%'])),\n",
        "                  index=pd.MultiIndex.from_product([['Testing    |', 'SurvTRACE    |', 'DeepHit    |', 'DSM         |'],['Breast','Heart']]))# ,\n",
        "dd.T.style.format(precision=4)\n",
        " "
      ],
      "metadata": {
        "id": "CYTsBrLSdGQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot Comparison to Other Methods\n",
        "ind = np.arange(3)\n",
        "width=.2\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
        "fig.subplots_adjust(hspace=.1, wspace=0.2)\n",
        "fig.set_size_inches(8, 3)\n",
        "plt.setp(axs, xticks=[0.2, 1.2, 2.2], xticklabels=['25%', '50%', '75%'])\n",
        "\n",
        "axs[0].bar(ind, met['seer'][0][0], width=width, label='Results')\n",
        "axs[0].bar(ind+width, seerb_c_paper, width=width, label='SurvTRACE')\n",
        "axs[0].bar(ind+2*width, deephitb, width=width, label='DeepHit')\n",
        "axs[0].bar(ind+3*width, dsmb, width=width, label='DSM')\n",
        "axs[0].set_title('SEER Brest Cancer Event')\n",
        "axs[0].set_ylabel('Concordance')\n",
        "axs[0].set_ylim([.5, 1])\n",
        "\n",
        "axs[1].bar(ind, met['seer'][0][1], width=width, label='Results')\n",
        "axs[1].bar(ind+width, seerh_c_paper, width=width, label='SurvTRACE')\n",
        "axs[1].bar(ind+width*2, deephith, width=width, label='DeepHit')\n",
        "axs[1].bar(ind+width*3, dsmh, width=width, label='DSM')\n",
        "axs[1].set_title('SEER Heart Disease Event')\n",
        "\n",
        "l = plt.legend(loc=\"lower left\", prop={'size': 8},\n",
        "               facecolor=\"white\", framealpha=1)\n"
      ],
      "metadata": {
        "id": "rl9Lh40TJ7V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter Grid Testing"
      ],
      "metadata": {
        "id": "DjZLrC6asuaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRID PARAMETER TESTING\n",
        "run_grid = False\n",
        "if run_grid:\n",
        "  EPOCHS = 20\n",
        "  BATCH_SIZE = 1280\n",
        "  LR = 1e-3  #learning rate default= 1e-3\n",
        "  WT_DECAY = 1e-4  #weight decay defalt le-4\n",
        "\n",
        "  grid_data = pd.DataFrame()\n",
        "  # #heads, embe_size, Intermediate_size, hidden_layers, \n",
        "  for heads in [2]:\n",
        "    STConfig.num_attention_heads = heads\n",
        "    for hid_size in [16]:\n",
        "      STConfig.hidden_size = hid_size\n",
        "      HIDDEN_SIZE = hid_size\n",
        "      for num_hid_lay in [2]:\n",
        "        STConfig.num_hidden_layers = num_hid_lay\n",
        "        for im_size in [32]:\n",
        "          STConfig.intermediate_size = im_size\n",
        "          for hid_drop in [0,.1,.2]:\n",
        "            STConfig.hidden_dropout_prob = hid_drop\n",
        "            for lrate in [1e-2, 1e-3, 1e-4]:\n",
        "              LR = lrate\n",
        "              print('---')\n",
        "              print('EPOCHS=', EPOCHS, 'BATCH=', BATCH_SIZE, 'LR=', lrate, 'Drop=', hid_drop)\n",
        "              print('HID SIZE', hid_size, 'HID_LAYERS', num_hid_lay, \"IM_SIZE\", im_size)\n",
        "              train_loader = torch.utils.data.DataLoader(CustomDataset(train_data), batch_size = BATCH_SIZE, shuffle=True)\n",
        "              val_loader = torch.utils.data.DataLoader(CustomDataset(val_data), batch_size = BATCH_SIZE, shuffle=False)\n",
        "\n",
        "              model = ST_Model().to(DEVICE)\n",
        "              train_loss_list, val_loss_list = train(model, train_loader, val_loader, weight_decay=WT_DECAY,learning_rate=LR, n_epochs=EPOCHS, verbose=False)\n",
        "              Eval = Evaluator1(df_train)\n",
        "              Eval.eval(model, x_test, df_test, brier=True)\n",
        "              print(' ')\n",
        "\n"
      ],
      "metadata": {
        "id": "QwM0N8cCeobN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}